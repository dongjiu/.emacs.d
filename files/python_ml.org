* installation
** pip
$ python -m pip install --upgrade pip
** numpy
$ pip install numpy
** scipy
$ pip install scipy
** sklearn
$ pip install -U scikit-learn
** matplotlib
$ pip install matplotlib
** pandas
$ pip install pandas

ImportError: Install xlrd >= 0.9.0 for Excel support
$ pip install xlrd
** SQLAlchemy
$ pip install SQLAlchemy==1.2.0

https://pypi.org/project/SQLAlchemy/1.2.0/
** tensorflow
$ pip install tensorflow
$ pip install tensorflow-gpu

*** error
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires
that this DLL be installed in a directory that is named in your %PATH%
environment variable. Download and install CUDA 9.0 from this URL:
https://developer.nvidia.com/cuda-toolkit
*** validate installation
#+BEGIN_SRC python
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
return sess.run(hello)
#+END_SRC

#+RESULTS:
: b'Hello, TensorFlow!'

** fancyimpute
https://pypi.org/project/fancyimpute/
$ pip install fancyimpute
*** Troubleshooting
**** gcc failed
$ sudo yum -y install python36u-devel

**** Failed building wheel for osqp. RuntimeError: CMake must be installed to build OSQP
$ sudo yum -y install cmake

update cmake:
$ cat /etc/*release
$ yum info cmake
$ sudo yum remove cmake -y
$ wget https://cmake.org/files/v3.6/cmake-3.6.2.tar.gz
$ tar -zxvf cmake-3.6.2.tar.gz
$ cd cmake-3.6.2
$ sudo ./bootstrap --prefix=/usr
$ sudo make
$ sudo make install
$ cmake --version

http://jotmynotes.blogspot.com/2016/10/updating-cmake-from-2811-to-362-or.html

**** No such file or directory: 'osqp_sources/build/out/libosqpstatic.a'
$ git clone https://github.com/oxfordcontrol/osqp
$ cd osqp
$ cmake -G "Unix Makefiles"
$ cmake --build .
$ sudo cmake --build . --target install
($ cmake --build . --target uninstall)

http://osqp.readthedocs.io/en/latest/installation/sources.html

** BeautifulSoup
sudo pip install beautifulsoup4
*** bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?
http://lxml.de/
sudo pip install lxml
** nltk
sudo pip install nltk

*** Resource mpunkt not found.  Please use the NLTK Downloader to obtain the resource:
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
#+END_SRC

#+RESULTS:
: None

*** stopwords
#+BEGIN_SRC python
import nltk
nltk.download('stopwords')
#+END_SRC

#+RESULTS:
: None
*** averaged_perceptron_tagger (use pos_tag)
#+BEGIN_SRC python
import nltk
nltk.download('averaged_perceptron_tagger')
#+END_SRC

#+RESULTS:
: None

*** brown
#+BEGIN_SRC python
import nltk
nltk.download('brown')
#+END_SRC

#+RESULTS:
: None

* install python 3 in centos
https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-centos-7

1. make sure yum is up to date
sudo yum -y update

2. install yum-utils
sudo yum -y install yum-utils

3. install CentOS development tools
sudo yum -y groupinstall development

4. install IUS
We would like to install the most current upstream stable release of
Python 3, we will need to install IUS, which stands for Inline with
Upstream Stable. A community project, IUS provides Red Hat Package
Manager (RPM) packages for some newer versions of select software.

sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpm

5. install python
sudo yum -y install python36u

6. check installation
python3.6 -V

7. install pip
sudo yum -y install python36u-pip

8. use pip to install packages
sudo pip3.6 install package_name
sudo pip3.6 install --upgrade pip
sudo pip install package_name

** matplotlib: ModuleNotFoundError: No module named 'tkinter'
sudo yum install python36u-tkinter
** opencv
sudo pip3.6 install opencv-python

* Loading data
scikit-learn comes with some common datasets we can quickly load.
** load_digits
load_digits contains 1797 observations from images of handwritten
digits. It is a good dataset for teaching image classification.

#+BEGIN_SRC python
  from sklearn import datasets

  # Load digits dataset
  digits = datasets.load_digits()

  # Create features matrix
  features = digits.data

  # Create target vector
  target = digits.target

  # View first observations
  return(features[0])
#+END_SRC

#+RESULTS:
| 0 | 0 | 5 | 13 | 9 | 1 | 0 | 0 | 0 | 0 | 13 | 15 | 10 | 15 | 5 | 0 | 0 | 3 | 15 | 2 | 0 | 11 | 8 | 0 | 0 | 4 | 12 | 0 | 0 | 8 | 8 | 0 | 0 | 5 | 8 | 0 | 0 | 9 | 8 | 0 | 0 | 4 | 11 | 0 | 1 | 12 | 7 | 0 | 0 | 2 | 14 | 5 | 10 | 12 | 0 | 0 | 0 | 0 | 6 | 13 | 10 | 0 | 0 | 0 |

** load_boston
load_boston contains 503 observations on Boston housing prices. It is
a good dataset for exploring regression algorithms.
** load_iris
load_iris contains 150 observations on the measurements of Iris
flowers. It is a good dataset for exploring classification algorithms.

** simulated dataset
scikit-learn offers many methods for creating simulated data.

When we want a dataset designed to be used with linear regression,
make_regression is a good choice.

#+BEGIN_SRC python
  from sklearn.datasets import make_regression
  
  # Generate features matrix, target vector, and the true coefficients
  features, target, coefficients = make_regression(n_samples = 100,
                                                   n_features = 3,
                                                   n_informative = 3,
                                                   n_targets = 1,
                                                   noise = 0.0,
                                                   coef = True,
                                                   random_state = 1)
  return (
      'Feature Matrix\n{}\nTarget Matrix\n{}'.format(features[:3], target[:3]))
#+END_SRC

#+RESULTS:
: Feature Matrix
: [[ 1.29322588 -0.61736206 -0.11044703]
:  [-2.793085    0.36633201  1.93752881]
:  [ 0.80186103 -0.18656977  0.0465673 ]]
: Target Matrix
: [-10.37865986  25.5124503   19.67705609]

If we are interested in creating a simulated dataset for
classification, we can use make_classification:

#+BEGIN_SRC python
  from sklearn.datasets import make_classification

  features, target = make_classification(n_samples = 100,
                                         n_features = 3,
                                         n_informative = 3,
                                         n_redundant = 0,
                                         n_classes = 2,
                                         weights = [.25, .75],
                                         random_state = 1)
  return('Feature Matrix\n{}\nTarget Vector\n{}'.format(features[:3], target[:3]))
#+END_SRC

#+RESULTS:
: Feature Matrix
: [[ 1.06354768 -1.42632219  1.02163151]
:  [ 0.23156977  1.49535261  0.33251578]
:  [ 0.15972951  0.83533515 -0.40869554]]
: Target Vector
: [1 0 0]

If we want a dataset designed to work well with clustering techniques,
scikit-learn offers make_blobs.

#+BEGIN_SRC python
  from sklearn.datasets import make_blobs
  import matplotlib.pyplot as plt

  features, target = make_blobs(n_samples = 100,
                                n_features = 2,
                                centers = 3,
                                cluster_std = 0.5,
                                shuffle = True,
                                random_state = 1)

  plt.scatter(features[:, 0], features[:, 1], c=target)
  plt.show()

  return('Feature Matrix\n{}\nTarget Vector\n{}'.format(features[:3], target[:3]))
#+END_SRC

#+RESULTS:
: Feature Matrix
: [[ -1.22685609   3.25572052]
:  [ -9.57463218  -4.38310652]
:  [-10.71976941  -4.20558148]]
: Target Vector
: [0 1 1]


In make_regression and make_classification, n_informative determines
the number of features that are used to generate the target vector. If
n_informative is less than the total number of features (n_features),
the resulting dataset will have redundant features that can be
identified through feature selection techniques.

In addition, make_classification contains a *weights* parameter that
allows us to simulate datasets with imbalanced classes.

For make_blobs, the *centers* parameter determines the number of
clusters generated. Using the matplotlib visualization library, we can
visualize the clusters generated by make_blobs.

** Loading a CSV File
Use the pandas library's read_csv to load a local or hosted CSV file.
[[http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html][read_csv spec]]

#+BEGIN_SRC python
  import pandas as pd
  import os
  
  #url = 'https://tinyurl.com/simulated_data'
  #dataframe = pd.read_csv(url)
  dataframe = pd.read_csv(os.path.join('data', 'test.csv'))
  
  return (dataframe.head(2))
#+END_SRC

#+RESULTS:
: integer              datetime   category
: 0        5   2018-01-01 00:00:00          0
: 1        3   2018-01-02 00:01:00          1

** Loading an Excel file
Use the pandas library's read_excel.

#+BEGIN_SRC python
  import pandas as pd
  import os
  
  dataframe = pd.read_excel(os.path.join('data', 'test.xlsx'), sheetname=0)
  
  return(dataframe.head(3))
#+END_SRC

#+RESULTS:
: emp_id first_name last_name onboard_date
: 0       1        Jim     Green   2012-09-01
: 1       2       Tony    Parker   2013-06-12
: 2       3        Tim    Duncan   2011-09-23

** Loading a JSON File
Use pandas's read_json.
[[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html][read_json spec]]
#+BEGIN_SRC python
  import pandas as pd
  import os

  dataframe = pd.read_json(os.path.join('data', 'test.json'), orient='index')
  return(dataframe.head(2))
#+END_SRC

#+RESULTS:
: col 1 col 2
: row 1     a     b
: row 2     c     d

** Querying a SQL Database
#+BEGIN_SRC python
  import pandas as pd
  from sqlalchemy import create_engine

  database_connection = create_engine('sqlite:///data/test.db')
  dataframe = pd.read_sql_query('SELECT * FROM person', database_connection)

  return (dataframe.head(2))
#+END_SRC

#+RESULTS:
: id first_name last_name
: 0   1        Tim    Duncan
: 1   2       Tony    Parker

* Data Wrangling
** Use DataFrame
[[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html][pandas.Series]]

#+BEGIN_SRC python
  import pandas as pd

  dataframe = pd.DataFrame()
  dataframe['Name'] = ['Jacky Jackson', 'Steven Stevenson']
  dataframe['Age'] = [38,25]
  dataframe['Driver'] = [True, False]

  # Append row
  new_person = pd.Series(['Molly Mooney', 40, True], index=['Name', 'Age', 'Driver'])
  dataframe = dataframe.append(new_person, ignore_index=True)

  return ('Head\n{}\nshape=\n{}\ndescribe:\n{}\niloc:\n{}'.format(
      dataframe.head(2),
      dataframe.shape,
      dataframe.describe(),
      dataframe.iloc[:2]))
#+END_SRC

#+RESULTS:
#+begin_example
Head
               Name  Age  Driver
0     Jacky Jackson   38    True
1  Steven Stevenson   25   False
shape=
(3, 3)
describe:
             Age
count   3.000000
mean   34.333333
std     8.144528
min    25.000000
25%    31.500000
50%    38.000000
75%    39.000000
max    40.000000
iloc:
               Name  Age  Driver
0     Jacky Jackson   38    True
1  Steven Stevenson   25   False
#+end_example

* Handling Numerical Data
** Rescaling a feature
#+BEGIN_SRC python
  import numpy as np
  from sklearn import preprocessing

  feature = np.array([[-500.5],[-100.1],[0],[100.1],[900.9]])

  minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))
  scaled_feature = minmax_scale.fit_transform(feature)
  return (scaled_feature)
#+END_SRC

#+RESULTS:
|          0 |
| 0.28571429 |
| 0.35714286 |
| 0.42857143 |
|          1 |
** Standardizing a Feature
#+BEGIN_SRC python
  import numpy as np
  from sklearn import preprocessing

  x = np.array([[-1000.1],[-200.2],[500.5],[600.6],[9000.9]])
  scaler = preprocessing.StandardScaler()
  standardized = scaler.fit_transform(x)

  robust_scaler = preprocessing.RobustScaler()
  robust = robust_scaler.fit_transform(x)

  return ('std:\n{}\nrobust:\n{}'.format(standardized, robust))
#+END_SRC

#+RESULTS:
#+begin_example
std:
[[-0.76058269]
 [-0.54177196]
 [-0.35009716]
 [-0.32271504]
 [ 1.97516685]]
robust:
[[-1.87387612]
 [-0.875     ]
 [ 0.        ]
 [ 0.125     ]
 [10.61488511]]
#+end_example

** Normalizing Observations
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import Normalizer

  # Create feature matrix
  features = np.array([[0.5, 0.5],
                       [1.1, 3.4],
                       [1.5, 20.2],
                       [1.63, 34.4],
                       [10.9, 3.3]])

  # Create normalizer
  normalizer = Normalizer(norm="l2")

  # Transform feature matrix
  return (normalizer.transform(features))
#+END_SRC

#+RESULTS:
| 0.70710678 | 0.70710678 |
| 0.30782029 | 0.95144452 |
| 0.07405353 | 0.99725427 |
| 0.04733062 | 0.99887928 |
| 0.95709822 | 0.28976368 |

** Transforming Features
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import FunctionTransformer

  features = np.array([[2,3],[2,3],[2,3]])

  def add_ten(x):
      return x+10

  ten_transformer = FunctionTransformer(add_ten)

  return ten_transformer.transform(features)
#+END_SRC

#+RESULTS:
| 12 | 13 |
| 12 | 13 |
| 12 | 13 |

** Detecting Outliers
#+BEGIN_SRC python
  import numpy as np
  from sklearn.covariance import EllipticEnvelope
  from sklearn.datasets import make_blobs

  # Create simulated data
  features, _ = make_blobs(n_samples = 10,
                           n_features = 2,
                           centers = 1,
                           random_state = 1)

  # Replace the first observation's values with extreme values
  features[0,0] = 10000
  features[0,1] = 10000

  # Create detector
  outlier_detector = EllipticEnvelope(contamination=.1)

  # Fit detector
  outlier_detector.fit(features)

  # Predict outliers
  return outlier_detector.predict(features)
#+END_SRC

#+RESULTS:
| -1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |


A major limitation of this approach is the need to specify a
/contamination/ parameter, which is the proportion of observations that
are outliers - a value that we don't know.

If we expect our data to have few outliers, we can set /contamination/
to something small.

** Interquartile range (IQR)
#+BEGIN_SRC python
  import numpy as np
  from sklearn.covariance import EllipticEnvelope
  from sklearn.datasets import make_blobs
  
  # Create simulated data
  features, _ = make_blobs(n_samples = 10,
                           n_features = 2,
                           centers = 1,
                           random_state = 1)
  
  # Replace the first observation's values with extreme values
  features[0,0] = 10000
  features[0,1] = 10000
  
  # Create one feature
  feature = features[:,0]
  
  # Create a function to return index of outliers
  def indicies_of_outliers(x):
      q1, q3 = np.percentile(x, [25, 75])
      iqr = q3 - q1
      lower_bound = q1 - (iqr * 1.5)
      upper_bound = q3 + (iqr * 1.5)
      return np.where((x > upper_bound) | (x < lower_bound))
  
  # Run function
  return indicies_of_outliers(feature)
#+END_SRC

#+RESULTS:
| array | ((0)) |

** Handling Outliers
*** Drop them
#+BEGIN_SRC python
import pandas as pd

# Create DataFrame
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]

# Filter observations
return houses[houses['Bathrooms'] < 20]
#+END_SRC

#+RESULTS:
: Price  Bathrooms  Square_Feet
: 0  534433        2.0         1500
: 1  392333        3.5         2500
: 2  293222        2.0         1500

*** Mark
#+BEGIN_SRC python
import pandas as pd
import numpy as np

# Create DataFrame
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]
houses["Outlier"] = np.where(houses["Bathrooms"] < 20, 0, 1)
return houses
#+END_SRC

#+RESULTS:
: Price  Bathrooms  Square_Feet  Outlier
: 0   534433        2.0         1500        0
: 1   392333        3.5         2500        0
: 2   293222        2.0         1500        0
: 3  4322032      116.0        48000        1

*** Transform the feature to dampen the effect of the outlier
#+BEGIN_SRC python
import pandas as pd
import numpy as np

# Create DataFrame
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]
houses['Log_Of_Square_Feet'] = [np.log(x) for x in houses["Square_Feet"]]

return houses
#+END_SRC

#+RESULTS:
: Price  Bathrooms  Square_Feet  Log_Of_Square_Feet
: 0   534433        2.0         1500            7.313220
: 1   392333        3.5         2500            7.824046
: 2   293222        2.0         1500            7.313220
: 3  4322032      116.0        48000           10.778956

** Discretizating Features
*** Binarize the feature
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import Binarizer

  # Create feature
  age = np.array([[6],
                  [12],
                  [20],
                  [36],
                  [65]])

  # Create binarizer
  binarizer = Binarizer(18)

  # Transform feature
  return binarizer.fit_transform(age)
#+END_SRC

#+RESULTS:
| 0 |
| 0 |
| 1 |
| 1 |
| 1 |

*** Multiple thresholds
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import Binarizer

  # Create feature
  age = np.array([[6],
                  [12],
                  [20],
                  [36],
                  [65]])

  # Create binarizer
  return np.digitize(age, bins=[20,30,64], right=True)
#+END_SRC

#+RESULTS:
| 0 |
| 0 |
| 0 |
| 2 |
| 3 |

** Grouping Observations Using Clustering
#+BEGIN_SRC python
  import pandas as pd
  from sklearn.datasets import make_blobs
  from sklearn.cluster import KMeans

  features, _ = make_blobs(n_samples = 50,
                           n_features = 2,
                           centers = 3,
                           random_state = 1)

  dataframe = pd.DataFrame(features, columns=["feature_1", "feature_2"])
  clusterer = KMeans(3, random_state=0)
  clusterer.fit(features)

  dataframe["group"] = clusterer.predict(features)

  return dataframe.head(5)
#+END_SRC

#+RESULTS:
: feature_1  feature_2  group
: 0  -9.877554  -3.336145      0
: 1  -7.287210  -8.353986      2
: 2  -6.943061  -7.023744      2
: 3  -7.440167  -8.791959      2
: 4  -6.641388  -8.075888      2

** Predict missing values using k-nearest neighbors (KNN) and scikit-learn's Imputer
#+BEGIN_SRC python
import numpy as np
from fancyimpute import KNN
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs
from sklearn.preprocessing import Imputer

# Make a simulated feature matrix
features, _ = make_blobs(n_samples = 1000,
                         n_features = 2,
                         random_state = 1)

# Standardize the features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Replace the first feature's first value with a missing value
true_value = standardized_features[0,0]
standardized_features[0,0] = np.nan

# Predict the missing values in the feature matrix
features_knn_imputed = KNN(k=5, verbose=0).complete(standardized_features)

mean_imputer = Imputer(strategy="mean", axis=0)
features_mean_imputed = mean_imputer.fit_transform(standardized_features)

# Compare true and imputed values
return ("True Value:\t{}\nKNN Imputed Value:\t{}\nsklearn Imputed: {}\n".format(true_value, features_knn_imputed[0,0], features_mean_imputed[0,0]))
#+END_SRC

#+RESULTS:
: True Value:	0.8730186113995938
: KNN Imputed Value:	1.0955332713113226
: sklearn Imputed: -0.000873892503901796

scikit-learns' Imputer module typically gets worse results than KNN.

* Handling Categorical Data
When the classes have no intrisic ordering, numerical values
erroneously create an ordering that is not present.

The proper way is to create a binary feature for each class in the
original feature. This is often called *one-hot encoding* (in machine
learning literature) or *dummying* (in statistical and research
literature).

In digital circuits, *one-hot* is a group of bits among which the legal
combinations of values are only those with a single high (1) bit and
all the others low (0). A similar implementation in which all bits
are '1' except one '0' is sometimes called *one-cold*.

** Encoding Nominal Categorical Features
#+BEGIN_SRC python
import numpy as np
from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer

feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
# create one-hot encoder
one_hot = LabelBinarizer()
return one_hot.fit_transform(feature)
#+END_SRC

#+RESULTS:
| 0 | 0 | 1 |
| 1 | 0 | 0 |
| 0 | 0 | 1 |
| 0 | 1 | 0 |
| 0 | 0 | 1 |

Output the classes.
#+BEGIN_SRC python
import numpy as np
from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer

feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
# create one-hot encoder
one_hot = LabelBinarizer()
one_hot.fit_transform(feature)
return one_hot.classes_
#+END_SRC

#+RESULTS:
| California | Delaware | Texas |

Reverse the one-hot encoding.
#+BEGIN_SRC python
import numpy as np
from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer

feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
# create one-hot encoder
one_hot = LabelBinarizer()
return one_hot.inverse_transform(one_hot.fit_transform(feature))
#+END_SRC

#+RESULTS:
| Texas | California | Texas | Delaware | Texas |


Use pandas to one-hot encode the feature.
#+BEGIN_SRC python
import numpy as np
import pandas as pd
feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
return pd.get_dummies(feature[:,0])
#+END_SRC

#+RESULTS:
: California  Delaware  Texas
: 0           0         0      1
: 1           1         0      0
: 2           0         0      1
: 3           0         1      0
: 4           0         0      1

Handle situations where each observation lists multiple classes.
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer
  
  feature = [("Texas", "Florida"),
             ("California", "Alabama"),
             ("Texas", "Florida"),
             ("Delaware", "Florida"),
             ("Texas", "Alabama")]
  # create one-hot encoder
  one_hot_multiclass = MultiLabelBinarizer()
  return "encoded:\n{}\nclasses:\n{}\n".format(one_hot_multiclass.fit_transform(feature), one_hot_multiclass.classes_)
#+END_SRC

#+RESULTS:
: encoded:
: [[0 0 0 1 1]
:  [1 1 0 0 0]
:  [0 0 0 1 1]
:  [0 0 1 1 0]
:  [1 0 0 0 1]]
: classes:
: ['Alabama' 'California' 'Delaware' 'Florida' 'Texas']

It is often recommended that after one-hot encoding a feature, we drop
one of the one-hot features in the resulting matrix to avoid linear
dependence.

** Encoding Ordinal Categorical Features
Often we have a feature with classes that have some kind of natural
ordering. A famous example is the *Likert scale*: Strongly Agree, Agree,
Neutral, Disagree, Strongly Disagree

Use pandas DataFrame's *replace* method to transform string labels to
numerical equivalents.

#+BEGIN_SRC python
import pandas as pd
dataframe = pd.DataFrame({"Score": ["Low", "Low", "Medium", "Medium", "High"]})
scale_mapper = {"Low": 1, "Medium": 2, "High": 3}
# replace feature values with scale
return dataframe["Score"].replace(scale_mapper)
#+END_SRC

#+RESULTS:
: 0    1
: 1    1
: 2    2
: 3    2
: 4    3
: Name: Score, dtype: int64

** Encoding Dictionaries of Features
Use *DictVectorizer*.
#+BEGIN_SRC python
  from sklearn.feature_extraction import DictVectorizer
  data_dict = [{"Red": 2, "Blue": 4},
               {"Red": 4, "Blue": 3},
               {"Red": 1, "Yellow": 2},
               {"Red": 2, "Yellow": 2}]
  dict_vectorizer = DictVectorizer(sparse=False)
  features = dict_vectorizer.fit_transform(data_dict)
  feature_names = dict_vectorizer.get_feature_names()
  return "features:\n{}\nnames:\n{}\n".format(features, feature_names)
#+END_SRC

#+RESULTS:
: features:
: [[4. 2. 0.]
:  [3. 4. 0.]
:  [0. 1. 2.]
:  [0. 2. 2.]]
: names:
: ['Blue', 'Red', 'Yellow']

** Imputing Missing Class Values
#+BEGIN_SRC python
  import numpy as np
  from sklearn.neighbors import KNeighborsClassifier
  from sklearn.preprocessing import Imputer
  
  X = np.array([[0, 2.10, 1.45],
                [1, 1.18, 1.33],
                [0, 1.22, 1.27],
                [1, -0.21, -1.19]])
  X_with_nan = np.array([[np.nan, 0.87, 1.31],
                         [np.nan, -0.67, -0.22]])
  
  # Train KNN learner
  clf = KNeighborsClassifier(3, weights="distance")
  trained_model = clf.fit(X[:, 1:], X[:,0])
  
  # Predict missing values' class
  imputed_values = trained_model.predict(X_with_nan[:, 1:])
  
  # Join column of predicted class with their other features
  X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:, 1:]))
  
  # Join two feature matrices
  knc = np.vstack((X_with_imputed, X))
  
  # Fill in missing values with the feature's most frequent value
  X_complete = np.vstack((X_with_nan, X))
  imputer = Imputer(strategy="most_frequent", axis=0)
  mfv = imputer.fit_transform(X_complete)
  
  return "KNeighborsClassifier:\n{}\nmost frequent values:\n{}\n".format(knc, mfv)
#+END_SRC

#+RESULTS:
#+begin_example
KNeighborsClassifier:
[[ 0.    0.87  1.31]
 [ 1.   -0.67 -0.22]
 [ 0.    2.1   1.45]
 [ 1.    1.18  1.33]
 [ 0.    1.22  1.27]
 [ 1.   -0.21 -1.19]]
most frequent values:
[[ 0.    0.87  1.31]
 [ 0.   -0.67 -0.22]
 [ 0.    2.1   1.45]
 [ 1.    1.18  1.33]
 [ 0.    1.22  1.27]
 [ 1.   -0.21 -1.19]]
#+end_example

** Handling Imbalanced Classes
Many algorithms in scikit-learn offer a prameter to weight classes
during training to counteract the effect of their imbalance.
#+BEGIN_SRC python
  import numpy as np
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.datasets import load_iris
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # remove first 40 observations
  features = features[40:,:]
  target = target[40:]
  
  # create binary target vector indicating if class 0
  target = np.where((target == 0), 0, 1)
  
  # use weights  
  weights = {0: .9, 1: 0.1}
  RandomForestClassifier(class_weight=weights)
  
  # create weights inversely proportional to class frequencies
  RandomForestClassifier(class_weight="balanced")
  
  # downsample the majority class
  i_class0 = np.where(target == 0)[0]
  i_class1 = np.where(target == 1)[0]
  n_class0 = len(i_class0)
  n_class1 = len(i_class1)
  i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)
  np.hstack((target[i_class0], target[i_class1_downsampled]))
  np.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]
  
  # upsample the minority class
  i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)
  np.concatenate((target[i_class0_upsampled], target[i_class1]))
  np.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]
#+END_SRC
*** downsample the majority class
#+BEGIN_SRC python
  import numpy as np
  from sklearn.datasets import load_iris
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # remove first 40 observations
  features = features[40:,:]
  target = target[40:]
  
  # create binary target vector indicating if class 0
  target = np.where((target == 0), 0, 1)
  
  # downsample the majority class
  i_class0 = np.where(target == 0)[0]
  i_class1 = np.where(target == 1)[0]
  n_class0 = len(i_class0)
  n_class1 = len(i_class1)
  i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)
  # join target vector
  target_vector = np.hstack((target[i_class0], target[i_class1_downsampled]))
  # join feature matrix
  feature_matrix = np.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]
  
  return "target vector:\n{}\nfeature matrix:\n{}\n".format(target_vector, feature_matrix)
#+END_SRC

#+RESULTS:
: target vector:
: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]
: feature matrix:
: [[5.  3.5 1.3 0.3]
:  [4.5 2.3 1.3 0.3]
:  [4.4 3.2 1.3 0.2]
:  [5.  3.5 1.6 0.6]
:  [5.1 3.8 1.9 0.4]]

*** upsample the minority class
#+BEGIN_SRC python
  import numpy as np
  from sklearn.datasets import load_iris
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # remove first 40 observations
  features = features[40:,:]
  target = target[40:]
  
  # create binary target vector indicating if class 0
  target = np.where((target == 0), 0, 1)

  i_class0 = np.where(target == 0)[0]
  i_class1 = np.where(target == 1)[0]
  n_class0 = len(i_class0)
  n_class1 = len(i_class1)
  
  i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)
  # join target vector
  target_vector = np.concatenate((target[i_class0_upsampled], target[i_class1]))
  # join feature matrix
  feature_matrix = np.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]
  return "target vector:\n{}\nfeature matrix:\n{}\n".format(target_vector, feature_matrix)  
#+END_SRC

#+RESULTS:
#+begin_example
  target vector:
  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
  feature matrix:
  [[4.6 3.2 1.4 0.2]
   [5.1 3.8 1.6 0.2]
   [4.4 3.2 1.3 0.2]
   [5.1 3.8 1.9 0.4]
   [5.  3.3 1.4 0.2]]
#+end_example

* Handling Text
** Cleaning Text
Use python's core string operations.
#+BEGIN_SRC python
  # Create text
  text_data = ["   Interrobang. By Aishwarya Henriette     ",
               "Parking And Going. By Karl Gautier",
               "    Today Is The night. By Jarek Prakash   "]
  # Strip whitespaces
  strip_whitespace = [string.strip() for string in text_data]
  
  def capitalizer(string: str) -> str:
      return string.upper()
  
  # Remove periods
  remove_periods = [capitalizer(string.replace(".","")) for string in strip_whitespace]
  
  return remove_periods
#+END_SRC

#+RESULTS:
| INTERROBANG BY AISHWARYA HENRIETTE | PARKING AND GOING BY KARL GAUTIER | TODAY IS THE NIGHT BY JAREK PRAKASH |

Regular expressions:
#+BEGIN_SRC python
  import re
  
  # Create text
  text_data = ["   Interrobang. By Aishwarya Henriette     ",
               "Parking And Going. By Karl Gautier",
               "    Today Is The night. By Jarek Prakash   "]
  # Strip whitespaces
  strip_whitespace = [string.strip() for string in text_data]
  
  # Remove periods
  remove_periods = [string.replace(".","") for string in strip_whitespace]
  
  def replace_letters_with_X(string: str) -> str:
      return re.sub(r"[a-zA-Z]", "X", string)
  
  replaced = [replace_letters_with_X(string) for string in remove_periods]
  return replaced
#+END_SRC

#+RESULTS:
| XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX | XXXXXXX XXX XXXXX XX XXXX XXXXXXX | XXXXX XX XXX XXXXX XX XXXXX XXXXXXX |

** Parsing and cleaning HTML
Use Beautiful Soup's extensive set of options to parse and extract from HTML.
https://www.crummy.com/software/BeautifulSoup/bs4/doc/>
#+BEGIN_SRC python
  from bs4 import BeautifulSoup
  
  html = """
  <div class='full_name'>
          <span style='font-weight:bold'>Masego</span> Azra
  </div>
  """
  
  soup = BeautifulSoup(html, "lxml")
  return soup.find("div", { "class": "full_name" }).text
#+END_SRC

#+RESULTS:
: Masego Azra

** Removing punctuation
Use *translate* with a dictionary of punctuation characters.
http://python-reference.readthedocs.io/en/latest/docs/str/translate.html
https://www.tutorialspoint.com/python/dictionary_fromkeys.htm
#+BEGIN_SRC python
  import unicodedata
  import sys
  
  text_data = ['Hi!!!! I. Love. This. Song....',
               '10000% Agree!!!! #LoveIT',
               'Right?!?!']
  punctuation = dict.fromkeys(i for i in range(sys.maxunicode)
                              if unicodedata.category(chr(i)).startswith('P'))
  
  translated = [string.translate(punctuation) for string in text_data]
  
  return "punctuation:\n{}\ntranslated:\n{}\n".format(punctuation, translated)
#+END_SRC

#+RESULTS:
: punctuation:
: {33: None, 34: None, 35: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 63: None, 64: None, 91: None, 92: None, 93: None, 95: None, 123: None, 125: None, 161: None, 167: None, 171: None, 182: None, 183: None, 187: None, 191: None, 894: None, 903: None, 1370: None, 1371: None, 1372: None, 1373: None, 1374: None, 1375: None, 1417: None, 1418: None, 1470: None, 1472: None, 1475: None, 1478: None, 1523: None, 1524: None, 1545: None, 1546: None, 1548: None, 1549: None, 1563: None, 1566: None, 1567: None, 1642: None, 1643: None, 1644: None, 1645: None, 1748: None, 1792: None, 1793: None, 1794: None, 1795: None, 1796: None, 1797: None, 1798: None, 1799: None, 1800: None, 1801: None, 1802: None, 1803: None, 1804: None, 1805: None, 2039: None, 2040: None, 2041: None, 2096: None, 2097: None, 2098: None, 2099: None, 2100: None, 2101: None, 2102: None, 2103: None, 2104: None, 2105: None, 2106: None, 2107: None, 2108: None, 2109: None, 2110: None, 2142: None, 2404: None, 2405: None, 2416: None, 2800: None, 3572: None, 3663: None, 3674: None, 3675: None, 3844: None, 3845: None, 3846: None, 3847: None, 3848: None, 3849: None, 3850: None, 3851: None, 3852: None, 3853: None, 3854: None, 3855: None, 3856: None, 3857: None, 3858: None, 3860: None, 3898: None, 3899: None, 3900: None, 3901: None, 3973: None, 4048: None, 4049: None, 4050: None, 4051: None, 4052: None, 4057: None, 4058: None, 4170: None, 4171: None, 4172: None, 4173: None, 4174: None, 4175: None, 4347: None, 4960: None, 4961: None, 4962: None, 4963: None, 4964: None, 4965: None, 4966: None, 4967: None, 4968: None, 5120: None, 5741: None, 5742: None, 5787: None, 5788: None, 5867: None, 5868: None, 5869: None, 5941: None, 5942: None, 6100: None, 6101: None, 6102: None, 6104: None, 6105: None, 6106: None, 6144: None, 6145: None, 6146: None, 6147: None, 6148: None, 6149: None, 6150: None, 6151: None, 6152: None, 6153: None, 6154: None, 6468: None, 6469: None, 6686: None, 6687: None, 6816: None, 6817: None, 6818: None, 6819: None, 6820: None, 6821: None, 6822: None, 6824: None, 6825: None, 6826: None, 6827: None, 6828: None, 6829: None, 7002: None, 7003: None, 7004: None, 7005: None, 7006: None, 7007: None, 7008: None, 7164: None, 7165: None, 7166: None, 7167: None, 7227: None, 7228: None, 7229: None, 7230: None, 7231: None, 7294: None, 7295: None, 7360: None, 7361: None, 7362: None, 7363: None, 7364: None, 7365: None, 7366: None, 7367: None, 7379: None, 8208: None, 8209: None, 8210: None, 8211: None, 8212: None, 8213: None, 8214: None, 8215: None, 8216: None, 8217: None, 8218: None, 8219: None, 8220: None, 8221: None, 8222: None, 8223: None, 8224: None, 8225: None, 8226: None, 8227: None, 8228: None, 8229: None, 8230: None, 8231: None, 8240: None, 8241: None, 8242: None, 8243: None, 8244: None, 8245: None, 8246: None, 8247: None, 8248: None, 8249: None, 8250: None, 8251: None, 8252: None, 8253: None, 8254: None, 8255: None, 8256: None, 8257: None, 8258: None, 8259: None, 8261: None, 8262: None, 8263: None, 8264: None, 8265: None, 8266: None, 8267: None, 8268: None, 8269: None, 8270: None, 8271: None, 8272: None, 8273: None, 8275: None, 8276: None, 8277: None, 8278: None, 8279: None, 8280: None, 8281: None, 8282: None, 8283: None, 8284: None, 8285: None, 8286: None, 8317: None, 8318: None, 8333: None, 8334: None, 8968: None, 8969: None, 8970: None, 8971: None, 9001: None, 9002: None, 10088: None, 10089: None, 10090: None, 10091: None, 10092: None, 10093: None, 10094: None, 10095: None, 10096: None, 10097: None, 10098: None, 10099: None, 10100: None, 10101: None, 10181: None, 10182: None, 10214: None, 10215: None, 10216: None, 10217: None, 10218: None, 10219: None, 10220: None, 10221: None, 10222: None, 10223: None, 10627: None, 10628: None, 10629: None, 10630: None, 10631: None, 10632: None, 10633: None, 10634: None, 10635: None, 10636: None, 10637: None, 10638: None, 10639: None, 10640: None, 10641: None, 10642: None, 10643: None, 10644: None, 10645: None, 10646: None, 10647: None, 10648: None, 10712: None, 10713: None, 10714: None, 10715: None, 10748: None, 10749: None, 11513: None, 11514: None, 11515: None, 11516: None, 11518: None, 11519: None, 11632: None, 11776: None, 11777: None, 11778: None, 11779: None, 11780: None, 11781: None, 11782: None, 11783: None, 11784: None, 11785: None, 11786: None, 11787: None, 11788: None, 11789: None, 11790: None, 11791: None, 11792: None, 11793: None, 11794: None, 11795: None, 11796: None, 11797: None, 11798: None, 11799: None, 11800: None, 11801: None, 11802: None, 11803: None, 11804: None, 11805: None, 11806: None, 11807: None, 11808: None, 11809: None, 11810: None, 11811: None, 11812: None, 11813: None, 11814: None, 11815: None, 11816: None, 11817: None, 11818: None, 11819: None, 11820: None, 11821: None, 11822: None, 11824: None, 11825: None, 11826: None, 11827: None, 11828: None, 11829: None, 11830: None, 11831: None, 11832: None, 11833: None, 11834: None, 11835: None, 11836: None, 11837: None, 11838: None, 11839: None, 11840: None, 11841: None, 11842: None, 11843: None, 11844: None, 12289: None, 12290: None, 12291: None, 12296: None, 12297: None, 12298: None, 12299: None, 12300: None, 12301: None, 12302: None, 12303: None, 12304: None, 12305: None, 12308: None, 12309: None, 12310: None, 12311: None, 12312: None, 12313: None, 12314: None, 12315: None, 12316: None, 12317: None, 12318: None, 12319: None, 12336: None, 12349: None, 12448: None, 12539: None, 42238: None, 42239: None, 42509: None, 42510: None, 42511: None, 42611: None, 42622: None, 42738: None, 42739: None, 42740: None, 42741: None, 42742: None, 42743: None, 43124: None, 43125: None, 43126: None, 43127: None, 43214: None, 43215: None, 43256: None, 43257: None, 43258: None, 43260: None, 43310: None, 43311: None, 43359: None, 43457: None, 43458: None, 43459: None, 43460: None, 43461: None, 43462: None, 43463: None, 43464: None, 43465: None, 43466: None, 43467: None, 43468: None, 43469: None, 43486: None, 43487: None, 43612: None, 43613: None, 43614: None, 43615: None, 43742: None, 43743: None, 43760: None, 43761: None, 44011: None, 64830: None, 64831: None, 65040: None, 65041: None, 65042: None, 65043: None, 65044: None, 65045: None, 65046: None, 65047: None, 65048: None, 65049: None, 65072: None, 65073: None, 65074: None, 65075: None, 65076: None, 65077: None, 65078: None, 65079: None, 65080: None, 65081: None, 65082: None, 65083: None, 65084: None, 65085: None, 65086: None, 65087: None, 65088: None, 65089: None, 65090: None, 65091: None, 65092: None, 65093: None, 65094: None, 65095: None, 65096: None, 65097: None, 65098: None, 65099: None, 65100: None, 65101: None, 65102: None, 65103: None, 65104: None, 65105: None, 65106: None, 65108: None, 65109: None, 65110: None, 65111: None, 65112: None, 65113: None, 65114: None, 65115: None, 65116: None, 65117: None, 65118: None, 65119: None, 65120: None, 65121: None, 65123: None, 65128: None, 65130: None, 65131: None, 65281: None, 65282: None, 65283: None, 65285: None, 65286: None, 65287: None, 65288: None, 65289: None, 65290: None, 65292: None, 65293: None, 65294: None, 65295: None, 65306: None, 65307: None, 65311: None, 65312: None, 65339: None, 65340: None, 65341: None, 65343: None, 65371: None, 65373: None, 65375: None, 65376: None, 65377: None, 65378: None, 65379: None, 65380: None, 65381: None, 65792: None, 65793: None, 65794: None, 66463: None, 66512: None, 66927: None, 67671: None, 67871: None, 67903: None, 68176: None, 68177: None, 68178: None, 68179: None, 68180: None, 68181: None, 68182: None, 68183: None, 68184: None, 68223: None, 68336: None, 68337: None, 68338: None, 68339: None, 68340: None, 68341: None, 68342: None, 68409: None, 68410: None, 68411: None, 68412: None, 68413: None, 68414: None, 68415: None, 68505: None, 68506: None, 68507: None, 68508: None, 69703: None, 69704: None, 69705: None, 69706: None, 69707: None, 69708: None, 69709: None, 69819: None, 69820: None, 69822: None, 69823: None, 69824: None, 69825: None, 69952: None, 69953: None, 69954: None, 69955: None, 70004: None, 70005: None, 70085: None, 70086: None, 70087: None, 70088: None, 70089: None, 70093: None, 70107: None, 70109: None, 70110: None, 70111: None, 70200: None, 70201: None, 70202: None, 70203: None, 70204: None, 70205: None, 70313: None, 70731: None, 70732: None, 70733: None, 70734: None, 70735: None, 70747: None, 70749: None, 70854: None, 71105: None, 71106: None, 71107: None, 71108: None, 71109: None, 71110: None, 71111: None, 71112: None, 71113: None, 71114: None, 71115: None, 71116: None, 71117: None, 71118: None, 71119: None, 71120: None, 71121: None, 71122: None, 71123: None, 71124: None, 71125: None, 71126: None, 71127: None, 71233: None, 71234: None, 71235: None, 71264: None, 71265: None, 71266: None, 71267: None, 71268: None, 71269: None, 71270: None, 71271: None, 71272: None, 71273: None, 71274: None, 71275: None, 71276: None, 71484: None, 71485: None, 71486: None, 72769: None, 72770: None, 72771: None, 72772: None, 72773: None, 72816: None, 72817: None, 74864: None, 74865: None, 74866: None, 74867: None, 74868: None, 92782: None, 92783: None, 92917: None, 92983: None, 92984: None, 92985: None, 92986: None, 92987: None, 92996: None, 113823: None, 121479: None, 121480: None, 121481: None, 121482: None, 121483: None, 125278: None, 125279: None}
: translated:
: ['Hi I Love This Song', '10000 Agree LoveIT', 'Right']

** Tokenizing text
Natural Language Toolkit for Python (NLTK) has a powerful set of text
manipulation operations, including word tokenizing.

#+BEGIN_SRC python
  from nltk.tokenize import word_tokenize
  from nltk.tokenize import sent_tokenize
  
  string = "The science of today is the technology of tomorrow. Tomorrow is today."
  return "words:\n{}\nsentences:\n{}\n".format(word_tokenize(string), sent_tokenize(string))
#+END_SRC

#+RESULTS:
: words:
: ['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow', '.', 'Tomorrow', 'is', 'today', '.']
: sentences:
: ['The science of today is the technology of tomorrow.', 'Tomorrow is today.']

** Removing stop words
#+BEGIN_SRC python
  from nltk.corpus import stopwords
  
  # You will have to download the set of stop words the first time
  # import nltk
  # nltk.download('stopwords')
  
  tokenzied_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']
  stop_words = stopwords.words('english')
  
  #remove stop words
  return [word for word in tokenzied_words if word not in stop_words]
#+END_SRC

#+RESULTS:
| going | go | store | park |

Note that NLTK's stopwords assumes the tokenized words are all lowercased.

** Stemming words
NLTK's PorterStemmer implements the widely used Porter stemming
algorithm to remove or replace common suffixes to produce the word
stem.
#+BEGIN_SRC python
  from nltk.stem.porter import PorterStemmer
  
  tokenzied_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']
  porter = PorterStemmer()
  return [porter.stem(word) for word in tokenzied_words]
#+END_SRC

#+RESULTS:
| i | am | humbl | by | thi | tradit | meet |


** Tagging parts of speech
Use NLTK's pre-trained parts-of-speech tagger.
#+BEGIN_SRC python
  from nltk import pos_tag
  from nltk import word_tokenize
  
  text_data = "Chris loverd outdoor running"
  text_tagged = pos_tag(word_tokenize(text_data))
  return text_tagged
#+END_SRC

#+RESULTS:
| Chris   | NNP |
| loverd  | JJ  |
| outdoor | NN  |
| running | VBG |

NLTK uses the Penn Treebank parts for speech tags. Some examples of
the Penn Treebank tags are.
| Tag | Part of speech                     |
|-----+------------------------------------|
| NNP | Proper noun, singular              |
| NN  | Noun, singular or mass             |
| RB  | Adverb                             |
| VBD | Verb, past tense                   |
| VBG | Verb, gerund or present participle |
| JJ  | Adjective                          |
| PRP | Personal pronoun                   |

#+BEGIN_SRC python
  from sklearn.preprocessing import MultiLabelBinarizer
  from nltk import pos_tag
  from nltk import word_tokenize
  
  tweets = ["I am eating a burrito for breakfast",
            "Political science is an amazing field",
            "San Francisco is an awesome city"]
  tagged_tweets = []
  for tweet in tweets:
      tweet_tag = pos_tag(word_tokenize(tweet))
      tagged_tweets.append([tag for word, tag in tweet_tag])
  
  one_hot_multi = MultiLabelBinarizer()
  features = one_hot_multi.fit_transform(tagged_tweets)
  return "features:\n{}\nclasses:\n{}\n".format(features, one_hot_multi.classes_)
#+END_SRC

#+RESULTS:
: features:
: [[1 1 0 1 0 1 1 1 0]
:  [1 0 1 1 0 0 0 0 1]
:  [1 0 1 1 1 0 0 0 1]]
: classes:
: ['DT' 'IN' 'JJ' 'NN' 'NNP' 'PRP' 'VBG' 'VBP' 'VBZ']

NLTK also gives us the ability to train our own tagger.
#+BEGIN_SRC python
  from nltk.corpus import brown
  from nltk.tag import UnigramTagger
  from nltk.tag import BigramTagger
  from nltk.tag import TrigramTagger
  
  sentences = brown.tagged_sents(categories='news')
  train = sentences[:4000]
  test = sentences[4000:]
  
  # create backoff tagger
  unigram = UnigramTagger(train)
  bigram = BigramTagger(train, backoff=unigram)
  trigram = TrigramTagger(train, backoff=bigram)
  
  # show accuracy
  return trigram.evaluate(test)
#+END_SRC

#+RESULTS:
: 0.8174734002697437

** Encoding text as a bag of words
Create a set of features indicating the number of times an
observation's text contains a particular word.

#+BEGIN_SRC python
  import numpy as np
  from sklearn.feature_extraction.text import CountVectorizer
  
  text_data = np.array(['I love Brazil. Brazil!',
                        'Sweden is best',
                        'Germany beats both'])
  # create a bag of words feature matrix
  count = CountVectorizer()
  bag_of_words = count.fit_transform(text_data)
  feature_names = count.get_feature_names()
  
  return "bag of words:\n{}\nto array:\n{}\nfeature names:\n{}\nvocabulary:\n{}\n".format(bag_of_words, bag_of_words.toarray(), feature_names, count.vocabulary_)
#+END_SRC

#+RESULTS:
#+begin_example
bag of words:
  (0, 3)	2
  (0, 6)	1
  (1, 1)	1
  (1, 5)	1
  (1, 7)	1
  (2, 2)	1
  (2, 0)	1
  (2, 4)	1
to array:
[[0 0 0 2 0 0 1 0]
 [0 1 0 0 0 1 0 1]
 [1 0 1 0 1 0 0 0]]
feature names:
['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']
vocabulary:
{'love': 6, 'brazil': 3, 'sweden': 7, 'is': 5, 'best': 1, 'germany': 4, 'beats': 0, 'both': 2}
#+end_example

Useful parameters of CountVectorizer.
#+BEGIN_SRC python
# Create feature matrix with arguments
count_2gram = CountVectorizer(ngram_range=(1,2),
                              stop_words="english",
                              vocabulary=['brazil'])
bag = count_2gram.fit_transform(text_data)

# View feature matrix
bag.toarray()
#+END_SRC

** Weighting word importance
Compare the frequency of the word in a document (a tweet, movie
review, speech transcript, etc.) with the frequency of the word in all
other documents using term frequency-inverse document frequency
(tf-idf).

#+BEGIN_SRC python
  import numpy as np
  from sklearn.feature_extraction.text import TfidfVectorizer
  
  text_data = np.array(['I love Brazil. Brazil!',
                        'Sweden is best',
                        'Germany beats both'])
  tfidf = TfidfVectorizer()
  feature_matrix = tfidf.fit_transform(text_data)
  return "feature matrix:\n{}\nvocabulary:\n{}\n".format(feature_matrix.toarray(), tfidf.vocabulary_)
#+END_SRC

#+RESULTS:
: feature matrix:
: [[0.         0.         0.         0.89442719 0.         0.
:   0.4472136  0.        ]
:  [0.         0.57735027 0.         0.         0.         0.57735027
:   0.         0.57735027]
:  [0.57735027 0.         0.57735027 0.         0.57735027 0.
:   0.         0.        ]]
: vocabulary:
: {'love': 6, 'brazil': 3, 'sweden': 7, 'is': 5, 'best': 1, 'germany': 4, 'beats': 0, 'both': 2}

The more a word appears in a document, the more likely it is important
to that document. For example, if the word /economy/ appears frequently,
it is evidence that the document might be about economics. We call
this *term frequency (tf)*.

In contrast, if a word appears in many documents, it is likely less
important to any individual document. For example, if every document
in some text data contains the word /after/ then it is probably an
unimportant word. We call this *document frequency (df)*.

By combining these two statistics, we can assign a score to every word
representing how important that word is in a document. Specifically,
we multiply *tf* to the inverse of document frequency (*idf*):

tf-idf(t,d)=tf(t,d) x idf(t)

where *t* is a word and *d* is a document. There are a number of
variations in how *tf* and *idf* are calculated. In scikit-learn, *tf* is
simply the number of times a word appears in the document and *idf* is
calculated as:

idf(t)=log[ (1+nd) / (1 + df(d,t))] + 1

where *nd* is the number of documents and *df(d,t)* is term, t's document
frequency (i.e., number of documents where the term appears).

By default, scikit-learn then normalizes the tf-idf vectors using the
Euclidean norm (L2 norm). The higher the resulting value, the more
important the word is to a document.

* Handling dates and times
** Converting strings to dates
#+BEGIN_SRC python
  import numpy as np
  import pandas as pd
  
  date_strings = np.array(['03-04-2018 11:35 PM',
                           '24-05-2018 12:01 AM',
                           '08-09-2017 09:09 PM'])
  dates = [pd.to_datetime(date, format='%d-%m-%Y %I:%M %p') for date in date_strings]
  return dates
#+END_SRC

#+RESULTS:
| Timestamp | (2018-04-03 23:35:00) | Timestamp | (2018-05-24 00:01:00) | Timestamp | (2017-09-08 21:09:00) |

Handling error.
#+BEGIN_SRC python
  import numpy as np
  import pandas as pd
  
  date_strings = np.array(['03-04-2018 11:35 PM',
                           '24-15-2018 12:01 AM',
                           '09-2017 09:09 PM'])
  dates = [pd.to_datetime(date, format='%d-%m-%Y %I:%M %p', errors="coerce") for date in date_strings]
  return dates
#+END_SRC

#+RESULTS:
| Timestamp | (2018-04-03 23:35:00) | NaT | NaT |

** Handling time zones
#+BEGIN_SRC python
import pandas as pd
return pd.Timestamp('2017-05-01 06:00:00', tz='Europe/London')
#+END_SRC

#+RESULTS:
: 2017-05-01 06:00:00+01:00

Add a time zone to a previously created datetime.
#+BEGIN_SRC python
import pandas as pd
date = pd.Timestamp('2017-05-01 06:00:00')
date_in_london = date.tz_localize('Europe/London')
date_in_abidjan = date_in_london.tz_convert('Africa/Abidjan')
return "date in London:\n{}\n\ndate in Abidjan:\n{}\n".format(date_in_london, date_in_abidjan)
#+END_SRC

#+RESULTS:
: date in London:
: 2017-05-01 06:00:00+01:00
: 
: date in Abidjan:
: 2017-05-01 05:00:00+00:00

Apply tz_localize and tz_convert to every element using Pandas' Servies object.
#+BEGIN_SRC python
import pandas as pd
dates = pd.Series(pd.date_range('2/2/2002', periods=3, freq='M'))
return dates.dt.tz_localize('Africa/Abidjan')
#+END_SRC

#+RESULTS:
: 0   2002-02-28 00:00:00+00:00
: 1   2002-03-31 00:00:00+00:00
: 2   2002-04-30 00:00:00+00:00
: dtype: datetime64[ns, Africa/Abidjan]

Show all time zone strings.
#+BEGIN_SRC python
from pytz import all_timezones
return all_timezones[0:5]
#+END_SRC

#+RESULTS:
| Africa/Abidjan | Africa/Accra | Africa/Addis_Ababa | Africa/Algiers | Africa/Asmara |


** Selecting dates and times
Use boolean conditions.
#+BEGIN_SRC python
  import pandas as pd
  
  dataframe = pd.DataFrame()
  dataframe['date'] = pd.date_range('1/1/2001', periods=100000, freq='H')
  
  return dataframe[(dataframe['date'] > '2002-1-1 01:00:00') &
                   (dataframe['date'] <= '2002-1-1 04:00:00')]
#+END_SRC

#+RESULTS:
: date
: 8762 2002-01-01 02:00:00
: 8763 2002-01-01 03:00:00
: 8764 2002-01-01 04:00:00

Use index.
#+BEGIN_SRC python
  import pandas as pd
  
  dataframe = pd.DataFrame()
  dataframe['date'] = pd.date_range('1/1/2001', periods=100000, freq='H')
  
  dataframe = dataframe.set_index(dataframe['date'])
  return dataframe.loc['2002-1-1 01:00:00':'2002-1-1 04:00:00']
#+END_SRC

#+RESULTS:
: date
: date                                   
: 2002-01-01 01:00:00 2002-01-01 01:00:00
: 2002-01-01 02:00:00 2002-01-01 02:00:00
: 2002-01-01 03:00:00 2002-01-01 03:00:00
: 2002-01-01 04:00:00 2002-01-01 04:00:00

** Breaking up date data into multiple features
#+BEGIN_SRC python
  import pandas as pd
  
  dataframe = pd.DataFrame()
  dataframe['date'] = pd.date_range('1/1/2001', periods=150, freq='W')
  
  dataframe['year'] = dataframe['date'].dt.year
  dataframe['month'] = dataframe['date'].dt.month
  dataframe['day'] = dataframe['date'].dt.day
  dataframe['hour'] = dataframe['date'].dt.hour
  dataframe['minute'] = dataframe['date'].dt.minute
  
  return dataframe.head(3)
#+END_SRC

#+RESULTS:
: date  year  month  day  hour  minute
: 0 2001-01-07  2001      1    7     0       0
: 1 2001-01-14  2001      1   14     0       0
: 2 2001-01-21  2001      1   21     0       0

** Calculating the difference between dates
#+BEGIN_SRC python
  import pandas as pd
  
  dataframe = pd.DataFrame()
  dataframe['Arrived'] = [pd.Timestamp('01-01-2017'), pd.Timestamp('01-04-2017')]
  dataframe['Left'] = [pd.Timestamp('01-01-2017'), pd.Timestamp('01-06-2017')]
  
  diff = dataframe['Left'] - dataframe['Arrived']
  duration = pd.Series(delta.days for delta in diff)
  return "diff:\n{}\n\nduration:\n{}\n".format(diff, duration)
#+END_SRC

#+RESULTS:
: diff:
: 0   0 days
: 1   2 days
: dtype: timedelta64[ns]
: 
: duration:
: 0    0
: 1    2
: dtype: int64

** Days of the week
#+BEGIN_SRC python
  import pandas as pd
  
  dates = pd.Series(pd.date_range('2/2/2002', periods=3, freq="M"))
  return "weekday_name:\n{}\n\nweekday:\n{}\n".format(dates.dt.weekday_name, dates.dt.weekday)
#+END_SRC

#+RESULTS:
#+begin_example
weekday_name:
0    Thursday
1      Sunday
2     Tuesday
dtype: object

weekday:
0    3
1    6
2    1
dtype: int64
#+end_example

** Creating a lagged feature
#+BEGIN_SRC python
  import pandas as pd
  
  dataframe = pd.DataFrame()
  dataframe['dates'] = pd.date_range('1/1/2001', periods=5, freq='D')
  dataframe['stock_price'] = [1.1,2.2,3.3,4.4,5.5]
  
  dataframe['previous_days_stock_price'] = dataframe['stock_price'].shift(1)
  return dataframe
#+END_SRC

#+RESULTS:
: dates  stock_price  previous_days_stock_price
: 0 2001-01-01          1.1                        NaN
: 1 2001-01-02          2.2                        1.1
: 2 2001-01-03          3.3                        2.2
: 3 2001-01-04          4.4                        3.3
: 4 2001-01-05          5.5                        4.4

** Using rolling time windows
#+BEGIN_SRC python
  import pandas as pd
  
  time_index = pd.date_range('01/01/2010', periods=5, freq='M')
  dataframe = pd.DataFrame(index=time_index)
  dataframe['stock_price'] = [1,2,3,4,5]
  return dataframe.rolling(window=2).mean()
#+END_SRC

#+RESULTS:
: stock_price
: 2010-01-31          NaN
: 2010-02-28          1.5
: 2010-03-31          2.5
: 2010-04-30          3.5
: 2010-05-31          4.5

** Handling missing data in time series
#+BEGIN_SRC python
  import pandas as pd
  import numpy as np
  
  time_index = pd.date_range('01/01/2010', periods=5, freq='M')
  dataframe = pd.DataFrame(index=time_index)
  dataframe['sales'] = [1.0, 2.0, np.nan, np.nan, 5.0]
  linear = dataframe.interpolate()
  ff = dataframe.ffill()
  bf = dataframe.bfill()
  quadratic = dataframe.interpolate(method="quadratic")
  advanced = dataframe.interpolate(limit=1, limit_direction='forward')
  return "linear:\n{}\nforward-filling:\n{}\nback-filling:\n{}\nquadratic:\n{}\nadvanced:\n{}\n".format(linear, ff, bf, quadratic, advanced)
#+END_SRC

#+RESULTS:
#+begin_example
linear:
            sales
2010-01-31    1.0
2010-02-28    2.0
2010-03-31    3.0
2010-04-30    4.0
2010-05-31    5.0
forward-filling:
            sales
2010-01-31    1.0
2010-02-28    2.0
2010-03-31    2.0
2010-04-30    2.0
2010-05-31    5.0
back-filling:
            sales
2010-01-31    1.0
2010-02-28    2.0
2010-03-31    5.0
2010-04-30    5.0
2010-05-31    5.0
quadratic:
               sales
2010-01-31  1.000000
2010-02-28  2.000000
2010-03-31  3.059808
2010-04-30  4.038069
2010-05-31  5.000000
advanced:
            sales
2010-01-31    1.0
2010-02-28    2.0
2010-03-31    3.0
2010-04-30    NaN
2010-05-31    5.0
#+end_example

* Handling images
** Loading images
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/myemail.jpg', cv2.IMREAD_GRAYSCALE)
  plt.imshow(image, cmap="gray"), plt.axis("off")
  plt.show()
  return "type: {}\nshape: {}\n".format(type(image), image.shape)
#+END_SRC

#+RESULTS:
: type: <class 'numpy.ndarray'>
: shape: (33, 272)

#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  # By default OpenCV uses BGR, but matplotlib uses RGB
  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  plt.imshow(image_rgb), plt.axis("off")
  plt.show()
  return "first pixel: BGR {}, RGB {}".format(image[0,0], image_rgb[0,0])
#+END_SRC

#+RESULTS:
: first pixel: BGR [103 112 121], RGB [121 112 103]

#+BEGIN_SRC python
  import cv2
  import numpy as np
  import os
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  if cv2.imwrite('data/tiger_copy.jpg', image):
      image2 = cv2.imread('data/tiger_copy.jpg', cv2.IMREAD_COLOR)
      plt.imshow(image2)
      plt.show()
      os.remove('data/tiger_copy.jpg')
#+END_SRC

#+RESULTS:
: None

** Resizing image
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  image_100x100 = cv2.resize(image, (100, 100))
  plt.imshow(image_100x100), plt.axis("off")
  plt.show()
  return image_100x100.shape
#+END_SRC

#+RESULTS:
| 100 | 100 | 3 |

** Cropping images
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  image_cropped = image[:, :128]
  plt.imshow(image_cropped), plt.axis("off")
  plt.show()
  return image_cropped.shape
#+END_SRC

#+RESULTS:
| 471 | 128 | 3 |

** Blurring images
Blur an image by averaging the values of a 5x5 kernel around each pixel.
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  image_blurry = cv2.blur(image, (5,5))
  plt.imshow(image_blurry), plt.axis("off")
  plt.show()
  return image_blurry.shape
#+END_SRC

#+RESULTS:
| 471 | 474 | 3 |

Use a kernel.
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  
  kernel = np.ones((5,5)) / 25.0
  image_kernel = cv2.filter2D(image, -1, kernel)
  
  plt.imshow(image_kernel), plt.xticks([]), plt.yticks([])
  plt.show()
  return kernel
#+END_SRC

#+RESULTS:
| 0.04 | 0.04 | 0.04 | 0.04 | 0.04 |
| 0.04 | 0.04 | 0.04 | 0.04 | 0.04 |
| 0.04 | 0.04 | 0.04 | 0.04 | 0.04 |
| 0.04 | 0.04 | 0.04 | 0.04 | 0.04 |
| 0.04 | 0.04 | 0.04 | 0.04 | 0.04 |

** Sharpening images
Construct a kernel to highlight the pixel itself.
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  
  kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
  image_sharp = cv2.filter2D(image, -1, kernel)
  
  plt.imshow(image_sharp), plt.xticks([]), plt.yticks([])
  plt.show()
  return kernel
#+END_SRC

#+RESULTS:
|  0 | -1 |  0 |
| -1 |  5 | -1 |
|  0 | -1 |  0 |

** Enhancing contrast
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  
  image_enhanced = cv2.equalizeHist(image_gray)
  
  plt.imshow(image_enhanced), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  # YUV color format
  # Y is the luminance (brightness) component
  # U and V are the chrominance (color) components
  image_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)
  image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0])
  image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB)
  
  plt.imshow(image_rgb), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None

** Isolating colors
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/tiger.jpg', cv2.IMREAD_COLOR)
  # hue, saturation, value
  image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
  lower_blue = np.array([50,100,50])
  upper_blue = np.array([130,255,255])
  
  mask = cv2.inRange(image_hsv, lower_blue, upper_blue)
  image_masked = cv2.bitwise_and(image, image, mask=mask)
  image_rgb = cv2.cvtColor(image_masked, cv2.COLOR_BGR2RGB)
  
  plt.imshow(image_rgb), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None

** Binarizing images
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/myemail.jpg', cv2.IMREAD_GRAYSCALE)
  
  max_output_value = 255
  neighborhood_size = 99
  subtract_from_mean = 10
  image_binarized = cv2.adaptiveThreshold(image,
                                          max_output_value,
                                          cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                          cv2.THRESH_BINARY,
                                          neighborhood_size,
                                          subtract_from_mean)
  plt.imshow(image_binarized, cmap="gray"), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None

** Removing backgrounds
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image_bgr = cv2.imread('data/tiger.jpg')
  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
  rect = (0, 56, 256, 150)
  
  # create initial mask
  mask = np.zeros(image_rgb.shape[:2], np.uint8)
  
  # create temporary arrays used by grabCut
  bgdModel = np.zeros((1,65), np.float64)
  fgdModel = np.zeros((1,65), np.float64)
  
  cv2.grabCut(image_rgb,
              mask,
              rect,
              bgdModel,
              fgdModel,
              5, # number of iterations
              cv2.GC_INIT_WITH_RECT)
  
  # create mask where sure and likely backgrounds set to 0, otherwise 1
  mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')
  
  # multiply image with new mask to subtract background
  image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]
  
  plt.imshow(image_rgb_nobg), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None

** Detecting edges
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image_gray = cv2.imread('data/myemail.jpg', cv2.IMREAD_GRAYSCALE)
  median_intensity = np.median(image_gray)
  lower_threshold = int(max(0, (1.0-0.33)*median_intensity))
  upper_threshold = int(max(255, (1.0+0.33)*median_intensity))

  # apply canny edge detector
  image_canny = cv2.Canny(image_gray, lower_threshold, upper_threshold)
  
  plt.imshow(image_canny, cmap="gray"), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None



** Detecting corners
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image_bgr = cv2.imread('data/tiger.jpg')
  image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
  image_gray = np.float32(image_gray)
  
  block_size = 2
  aperture = 29
  free_parameter = 0.04
  
  detector_responses = cv2.cornerHarris(image_gray,
                                        block_size,
                                        aperture,
                                        free_parameter)
  # large corner markers
  detector_responses = cv2.dilate(detector_responses, None)
  
  # only keep detector responses greater than threshold, mark as white
  threshold = 0.02
  image_bgr[detector_responses > threshold * detector_responses.max()] = [255,255,255]
  image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
  
  plt.imshow(image_gray, cmap="gray"), plt.axis("off")
  plt.show()
#+END_SRC

#+RESULTS:
: None

** Creating features for machine learning
Use Numpy's *flatten* to convert the multidimensional array containing
an image's data into a vector containing the observation's values.

#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image = cv2.imread('data/myemail.jpg', cv2.IMREAD_GRAYSCALE)
  image_2x5 = cv2.resize(image, (2,5))
  return image_2x5.flatten()
#+END_SRC

#+RESULTS:
| 253 | 255 | 254 | 253 | 205 | 255 | 253 | 111 | 255 | 255 |

** Encoding mean color as a feature
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image_bgr = cv2.imread('data/tiger.jpg')
  channels = cv2.mean(image_bgr)
  # rgb
  observation = np.array([(channels[2], channels[1], channels[0])])
  return observation
#+END_SRC

#+RESULTS:
| 117.31358005 | 106.15488188 | 87.6743261 |

** Encoding color histograms as features
#+BEGIN_SRC python
  import cv2
  import numpy as np
  from matplotlib import pyplot as plt
  
  image_bgr = cv2.imread('data/tiger.jpg')
  image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
  features = []
  colors = ('r', 'g', 'b')
  # for each channel, calculate histogram and add to feature value list
  for i, channel in enumerate(colors):
      histogram = cv2.calcHist([image_rgb],
                               [i], # index of channel
                               None, # no mask
                               [256], # histogram size
                               [0, 256]) # range
      features.extend(histogram)
      plt.plot(histogram, color=channel)
      plt.xlim([0,256])
  plt.show()
  observation = np.array(features).flatten()
  return observation[0:5]
#+END_SRC

#+RESULTS:
| 482 | 578 | 331 | 692 | 694 |

* Dimensionality reduction using feature extraction
** PCA
#+BEGIN_SRC python
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn import datasets
  
  digits = datasets.load_digits()
  
  # standardize the feature matrix
  features = StandardScaler().fit_transform(digits.data)
  
  # create a PCA that will retain 99% of variance
  # whiten=True transforms the values of each principal component
  # so that they have zero mean and unit variance.
  pca = PCA(n_components=0.99, whiten=True)
  
  # conduct PCA
  features_pca = pca.fit_transform(features)
  
  return "original number of features: {}, reduced number of features: {}\n".format(features.shape[1], features_pca.shape[1])
#+END_SRC

#+RESULTS:
: original number of features: 64, reduced number of features: 54

** Reducing features when data is linearly inseparable
Use an extension of PCA that uses kernels to allow for non-linear
dimensionality reduction.
#+BEGIN_SRC python
  from sklearn.decomposition import PCA, KernelPCA
  from sklearn.datasets import make_circles
  # make_circles makes linearly inseparable data;
  # specifically, one class is surrounded on all sides by the other class
  features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)
  # apply kernel PCA with radius basis function (RBF) kernel
  kpca = KernelPCA(kernel="rbf", gamma=15, n_components=1)
  features_kpca = kpca.fit_transform(features)
  return "original number of features: {}, reduced number of features: {}\n".format(features.shape[1], features_kpca.shape[1])
#+END_SRC

#+RESULTS:
: original number of features: 2, reduced number of features: 1

** Reducing features by maximizing class separability
#+BEGIN_SRC python
  from sklearn import datasets
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  
  iris = datasets.load_iris()
  features = iris.data
  target = iris.target
  
  lda = LinearDiscriminantAnalysis(n_components=1)
  features_lda = lda.fit(features, target).transform(features)
  
  return "Original number of features: {}, reduced: {}\nexplained variance ratio: {}\n".format(features.shape[1], features_lda.shape[1], lda.explained_variance_ratio_)
#+END_SRC

#+RESULTS:
: Original number of features: 4, reduced: 1
: explained variance ratio: [0.99147248]

** Reducing features using matrix factorization
Use non-negative matrix factorization (NMF).
#+BEGIN_SRC python
  from sklearn.decomposition import NMF
  from sklearn import datasets
  
  digits = datasets.load_digits()
  features = digits.data
  
  nmf = NMF(n_components=10, random_state=1)
  features_nmf = nmf.fit_transform(features)
  
  return "origin features: {}, reduced: {}\n".format(features.shape[1], features_nmf.shape[1])
#+END_SRC

#+RESULTS:
: origin features: 64, reduced: 10

** Reducing features on sparse data
Use truncated singular value decomposition (TSVD).
#+BEGIN_SRC python
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import TruncatedSVD
  from scipy.sparse import csr_matrix
  from sklearn import datasets
  import numpy as np
  
  digits = datasets.load_digits()
  features = StandardScaler().fit_transform(digits.data)
  features_sparse = csr_matrix(features)
  tsvd = TruncatedSVD(n_components=10)
  features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)
  return "origin: {}, reduced: {}\n".format(features_sparse.shape[1], features_sparse_tsvd.shape[1])
#+END_SRC

#+RESULTS:
: origin: 64, reduced: 10

* Dimensionality Reduction Using Feature Selection
** Thresholding numerical feature variance
You have a set of numerical features and want to remove those with low
variance (i.e., likely containing little information).

Variance thresholding will not work correctly if the features have
been standardized (to mean zero and unit variance).

#+BEGIN_SRC python
  from sklearn import datasets
  from sklearn.feature_selection import VarianceThreshold
  
  iris = datasets.load_iris()
  features = iris.data
  target = iris.target
  thresholder = VarianceThreshold(threshold=.5)
  features_high_variance = thresholder.fit_transform(features)
  return features_high_variance[0:3]
#+END_SRC

#+RESULTS:
| 5.1 | 1.4 | 0.2 |
| 4.9 | 1.4 | 0.2 |
| 4.7 | 1.3 | 0.2 |

** Thresholding binary feature variance
You have a set of binary features and want to remove those with low
variance.

In binary features (i.e., Bernoulli random variables), variance is
calculated as:
Var(x) = p(1-p)
where p is the proportion of observations of class 1.

#+BEGIN_SRC python
  from sklearn.feature_selection import VarianceThreshold
  
  features = [[0,1,0], [0,1,1], [0,1,0], [0,1,1], [1,0,0]]
  thresholder = VarianceThreshold(threshold=(.75*.25))
  return thresholder.fit_transform(features)
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 0 |
| 1 |
| 0 |

** Handling highly correlated features
Use a correlation matrix to check for highly correlated features. If
highly correlated features exist, consider dropping one of the
correlated features.

#+BEGIN_SRC python
  import pandas as pd
  import numpy as np
  
  features = np.array([[1,1,1],
                       [2,2,0],
                       [3,3,1],
                       [4,4,0],
                       [5,5,1],
                       [6,6,0],
                       [7,7,1],
                       [8,7,0],
                       [9,7,1]])
  dataframe = pd.DataFrame(features)
  # create correlation matrix
  corr_matrix = dataframe.corr().abs()
  # select upper triangle of correlation matrix
  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
  # find index of feature columns with correlation greater than 0.95
  to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
  return dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)
#+END_SRC

#+RESULTS:
: 0  2
: 0  1  1
: 1  2  0
: 2  3  1

** Removing irrelevant features for classification
If the features are categorical, calculate a chi-square statistics
between each feature and the target vector.

#+BEGIN_SRC python
  from sklearn.datasets import load_iris
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2, f_classif
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # convert to categorical data by converting data to integers
  features = features.astype(int)
  
  # select two features with highest chi-squared statistics
  chi2_selector = SelectKBest(chi2, k=2)
  features_kbest = chi2_selector.fit_transform(features, target)
  
  return "origin number of features: {}, reduced: {}\n".format(features.shape[1], features_kbest.shape[1])
#+END_SRC

#+RESULTS:
: origin number of features: 4, reduced: 2


If the features are quantitative, compute the ANOVA F-value between
each feature and target vector.

#+BEGIN_SRC python
  from sklearn.datasets import load_iris
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2, f_classif
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  fvalue_selector = SelectKBest(f_classif, k=2)
  features_kbest = fvalue_selector.fit_transform(features, target)
  
  return "origin number of features: {}, reduced: {}\n".format(features.shape[1], features_kbest.shape[1])
#+END_SRC

#+RESULTS:
: origin number of features: 4, reduced: 2

Instead of selecting a specific number of features, we can also use
SelectPercentile to select the top n percent of features.

#+BEGIN_SRC python
  from sklearn.datasets import load_iris
  from sklearn.feature_selection import SelectPercentile
  from sklearn.feature_selection import chi2, f_classif
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  fvalue_selector = SelectPercentile(f_classif, percentile=75)
  features_kbest = fvalue_selector.fit_transform(features, target)
  
  return "origin number of features: {}, reduced: {}\n".format(features.shape[1], features_kbest.shape[1])
#+END_SRC

#+RESULTS:
: origin number of features: 4, reduced: 3

** Recursively eliminating features
Use scikit-learn's RFECV to conduct recursive feature elimination
(RFE) using cross-validation (CV). That is, repeatedly train a model,
each time removing a feature until model performance (e.g., accuracy)
becomes worse. The remaining features are the best.

#+BEGIN_SRC python
  import warnings
  from sklearn.datasets import make_regression
  from sklearn.feature_selection import RFECV
  from sklearn import datasets, linear_model
  
  # suppress an annoying but harmless warning
  warnings.filterwarnings(action="ignore", module="scipy", message="^internal gelsd")
  features, target = make_regression(n_samples=10000,
                                     n_features=100,
                                     n_informative=2,
                                     random_state=1)
  ols = linear_model.LinearRegression()
  
  rfecv = RFECV(estimator=ols, step=1, scoring="neg_mean_squared_error")
  rfecv.fit(features, target)
  rfecv.transform(features)
  return features.shape
#+END_SRC

#+RESULTS:
| 10000 | 100 |

* Model Evaluation
