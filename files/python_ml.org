* installation
** pip
$ python -m pip install --upgrade pip
** numpy
$ pip install numpy
** scipy
$ pip install scipy
** sklearn
$ pip install -U scikit-learn
** matplotlib
$ pip install matplotlib
** pandas
$ pip install pandas

ImportError: Install xlrd >= 0.9.0 for Excel support
$ pip install xlrd
** SQLAlchemy
$ pip install SQLAlchemy==1.2.0

https://pypi.org/project/SQLAlchemy/1.2.0/
** tensorflow
$ pip install tensorflow
$ pip install tensorflow-gpu

*** error
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires
that this DLL be installed in a directory that is named in your %PATH%
environment variable. Download and install CUDA 9.0 from this URL:
https://developer.nvidia.com/cuda-toolkit
*** validate installation
#+BEGIN_SRC python
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
return sess.run(hello)
#+END_SRC

#+RESULTS:
: b'Hello, TensorFlow!'

** fancyimpute
https://pypi.org/project/fancyimpute/
$ pip install fancyimpute
*** Troubleshooting
**** gcc failed
$ sudo yum -y install python36u-devel

**** Failed building wheel for osqp. RuntimeError: CMake must be installed to build OSQP
$ sudo yum -y install cmake

update cmake:
$ cat /etc/*release
$ yum info cmake
$ sudo yum remove cmake -y
$ wget https://cmake.org/files/v3.6/cmake-3.6.2.tar.gz
$ tar -zxvf cmake-3.6.2.tar.gz
$ cd cmake-3.6.2
$ sudo ./bootstrap --prefix=/usr
$ sudo make
$ sudo make install
$ cmake --version

http://jotmynotes.blogspot.com/2016/10/updating-cmake-from-2811-to-362-or.html

**** No such file or directory: 'osqp_sources/build/out/libosqpstatic.a'
$ git clone https://github.com/oxfordcontrol/osqp
$ cd osqp
$ cmake -G "Unix Makefiles"
$ cmake --build .
$ sudo cmake --build . --target install
($ cmake --build . --target uninstall)

http://osqp.readthedocs.io/en/latest/installation/sources.html

** BeautifulSoup
sudo pip install beautifulsoup4
*** bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?
http://lxml.de/
sudo pip install lxml
** nltk
sudo pip install nltk

*** Resource mpunkt not found.  Please use the NLTK Downloader to obtain the resource:
#+BEGIN_SRC python
import nltk
nltk.download('punkt')
#+END_SRC

#+RESULTS:
: None

*** stopwords
#+BEGIN_SRC python
import nltk
nltk.download('stopwords')
#+END_SRC

#+RESULTS:
: None
*** averaged_perceptron_tagger (use pos_tag)
#+BEGIN_SRC python
import nltk
nltk.download('averaged_perceptron_tagger')
#+END_SRC

#+RESULTS:
: None

*** brown
#+BEGIN_SRC python
import nltk
nltk.download('brown')
#+END_SRC

#+RESULTS:
: None

* install python 3 in centos
https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-centos-7

1. make sure yum is up to date
sudo yum -y update

2. install yum-utils
sudo yum -y install yum-utils

3. install CentOS development tools
sudo yum -y groupinstall development

4. install IUS
We would like to install the most current upstream stable release of
Python 3, we will need to install IUS, which stands for Inline with
Upstream Stable. A community project, IUS provides Red Hat Package
Manager (RPM) packages for some newer versions of select software.

sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpm

5. install python
sudo yum -y install python36u

6. check installation
python3.6 -V

7. install pip
sudo yum -y install python36u-pip

8. use pip to install packages
sudo pip3.6 install package_name
sudo pip3.6 install --upgrade pip
sudo pip install package_name

** matplotlib: ModuleNotFoundError: No module named 'tkinter'
sudo yum install python36u-tkinter
* Loading data
scikit-learn comes with some common datasets we can quickly load.
** load_digits
load_digits contains 1797 observations from images of handwritten
digits. It is a good dataset for teaching image classification.

#+BEGIN_SRC python
  from sklearn import datasets

  # Load digits dataset
  digits = datasets.load_digits()

  # Create features matrix
  features = digits.data

  # Create target vector
  target = digits.target

  # View first observations
  return(features[0])
#+END_SRC

#+RESULTS:
| 0 | 0 | 5 | 13 | 9 | 1 | 0 | 0 | 0 | 0 | 13 | 15 | 10 | 15 | 5 | 0 | 0 | 3 | 15 | 2 | 0 | 11 | 8 | 0 | 0 | 4 | 12 | 0 | 0 | 8 | 8 | 0 | 0 | 5 | 8 | 0 | 0 | 9 | 8 | 0 | 0 | 4 | 11 | 0 | 1 | 12 | 7 | 0 | 0 | 2 | 14 | 5 | 10 | 12 | 0 | 0 | 0 | 0 | 6 | 13 | 10 | 0 | 0 | 0 |

** load_boston
load_boston contains 503 observations on Boston housing prices. It is
a good dataset for exploring regression algorithms.
** load_iris
load_iris contains 150 observations on the measurements of Iris
flowers. It is a good dataset for exploring classification algorithms.

** simulated dataset
scikit-learn offers many methods for creating simulated data.

When we want a dataset designed to be used with linear regression,
make_regression is a good choice.

#+BEGIN_SRC python
  from sklearn.datasets import make_regression
  
  # Generate features matrix, target vector, and the true coefficients
  features, target, coefficients = make_regression(n_samples = 100,
                                                   n_features = 3,
                                                   n_informative = 3,
                                                   n_targets = 1,
                                                   noise = 0.0,
                                                   coef = True,
                                                   random_state = 1)
  return (
      'Feature Matrix\n{}\nTarget Matrix\n{}'.format(features[:3], target[:3]))
#+END_SRC

#+RESULTS:
: Feature Matrix
: [[ 1.29322588 -0.61736206 -0.11044703]
:  [-2.793085    0.36633201  1.93752881]
:  [ 0.80186103 -0.18656977  0.0465673 ]]
: Target Matrix
: [-10.37865986  25.5124503   19.67705609]

If we are interested in creating a simulated dataset for
classification, we can use make_classification:

#+BEGIN_SRC python
  from sklearn.datasets import make_classification

  features, target = make_classification(n_samples = 100,
                                         n_features = 3,
                                         n_informative = 3,
                                         n_redundant = 0,
                                         n_classes = 2,
                                         weights = [.25, .75],
                                         random_state = 1)
  return('Feature Matrix\n{}\nTarget Vector\n{}'.format(features[:3], target[:3]))
#+END_SRC

#+RESULTS:
: Feature Matrix
: [[ 1.06354768 -1.42632219  1.02163151]
:  [ 0.23156977  1.49535261  0.33251578]
:  [ 0.15972951  0.83533515 -0.40869554]]
: Target Vector
: [1 0 0]

If we want a dataset designed to work well with clustering techniques,
scikit-learn offers make_blobs.

#+BEGIN_SRC python
  from sklearn.datasets import make_blobs
  import matplotlib.pyplot as plt

  features, target = make_blobs(n_samples = 100,
                                n_features = 2,
                                centers = 3,
                                cluster_std = 0.5,
                                shuffle = True,
                                random_state = 1)

  plt.scatter(features[:, 0], features[:, 1], c=target)
  plt.show()

  return('Feature Matrix\n{}\nTarget Vector\n{}'.format(features[:3], target[:3]))
#+END_SRC

#+RESULTS:
: Feature Matrix
: [[ -1.22685609   3.25572052]
:  [ -9.57463218  -4.38310652]
:  [-10.71976941  -4.20558148]]
: Target Vector
: [0 1 1]


In make_regression and make_classification, n_informative determines
the number of features that are used to generate the target vector. If
n_informative is less than the total number of features (n_features),
the resulting dataset will have redundant features that can be
identified through feature selection techniques.

In addition, make_classification contains a *weights* parameter that
allows us to simulate datasets with imbalanced classes.

For make_blobs, the *centers* parameter determines the number of
clusters generated. Using the matplotlib visualization library, we can
visualize the clusters generated by make_blobs.

** Loading a CSV File
Use the pandas library's read_csv to load a local or hosted CSV file.
[[http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html][read_csv spec]]

#+BEGIN_SRC python
  import pandas as pd
  import os
  
  #url = 'https://tinyurl.com/simulated_data'
  #dataframe = pd.read_csv(url)
  dataframe = pd.read_csv(os.path.join('data', 'test.csv'))
  
  return (dataframe.head(2))
#+END_SRC

#+RESULTS:
: integer              datetime   category
: 0        5   2018-01-01 00:00:00          0
: 1        3   2018-01-02 00:01:00          1

** Loading an Excel file
Use the pandas library's read_excel.

#+BEGIN_SRC python
  import pandas as pd
  import os
  
  dataframe = pd.read_excel(os.path.join('data', 'test.xlsx'), sheetname=0)
  
  return(dataframe.head(3))
#+END_SRC

#+RESULTS:
: emp_id first_name last_name onboard_date
: 0       1        Jim     Green   2012-09-01
: 1       2       Tony    Parker   2013-06-12
: 2       3        Tim    Duncan   2011-09-23

** Loading a JSON File
Use pandas's read_json.
[[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html][read_json spec]]
#+BEGIN_SRC python
  import pandas as pd
  import os

  dataframe = pd.read_json(os.path.join('data', 'test.json'), orient='index')
  return(dataframe.head(2))
#+END_SRC

#+RESULTS:
: col 1 col 2
: row 1     a     b
: row 2     c     d

** Querying a SQL Database
#+BEGIN_SRC python
  import pandas as pd
  from sqlalchemy import create_engine

  database_connection = create_engine('sqlite:///data/test.db')
  dataframe = pd.read_sql_query('SELECT * FROM person', database_connection)

  return (dataframe.head(2))
#+END_SRC

#+RESULTS:
: id first_name last_name
: 0   1        Tim    Duncan
: 1   2       Tony    Parker

* Data Wrangling
** Use DataFrame
[[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html][pandas.Series]]

#+BEGIN_SRC python
  import pandas as pd

  dataframe = pd.DataFrame()
  dataframe['Name'] = ['Jacky Jackson', 'Steven Stevenson']
  dataframe['Age'] = [38,25]
  dataframe['Driver'] = [True, False]

  # Append row
  new_person = pd.Series(['Molly Mooney', 40, True], index=['Name', 'Age', 'Driver'])
  dataframe = dataframe.append(new_person, ignore_index=True)

  return ('Head\n{}\nshape=\n{}\ndescribe:\n{}\niloc:\n{}'.format(
      dataframe.head(2),
      dataframe.shape,
      dataframe.describe(),
      dataframe.iloc[:2]))
#+END_SRC

#+RESULTS:
#+begin_example
Head
               Name  Age  Driver
0     Jacky Jackson   38    True
1  Steven Stevenson   25   False
shape=
(3, 3)
describe:
             Age
count   3.000000
mean   34.333333
std     8.144528
min    25.000000
25%    31.500000
50%    38.000000
75%    39.000000
max    40.000000
iloc:
               Name  Age  Driver
0     Jacky Jackson   38    True
1  Steven Stevenson   25   False
#+end_example

* Handling Numerical Data
** Rescaling a feature
#+BEGIN_SRC python
  import numpy as np
  from sklearn import preprocessing

  feature = np.array([[-500.5],[-100.1],[0],[100.1],[900.9]])

  minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))
  scaled_feature = minmax_scale.fit_transform(feature)
  return (scaled_feature)
#+END_SRC

#+RESULTS:
|          0 |
| 0.28571429 |
| 0.35714286 |
| 0.42857143 |
|          1 |
** Standardizing a Feature
#+BEGIN_SRC python
  import numpy as np
  from sklearn import preprocessing

  x = np.array([[-1000.1],[-200.2],[500.5],[600.6],[9000.9]])
  scaler = preprocessing.StandardScaler()
  standardized = scaler.fit_transform(x)

  robust_scaler = preprocessing.RobustScaler()
  robust = robust_scaler.fit_transform(x)

  return ('std:\n{}\nrobust:\n{}'.format(standardized, robust))
#+END_SRC

#+RESULTS:
#+begin_example
std:
[[-0.76058269]
 [-0.54177196]
 [-0.35009716]
 [-0.32271504]
 [ 1.97516685]]
robust:
[[-1.87387612]
 [-0.875     ]
 [ 0.        ]
 [ 0.125     ]
 [10.61488511]]
#+end_example

** Normalizing Observations
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import Normalizer

  # Create feature matrix
  features = np.array([[0.5, 0.5],
                       [1.1, 3.4],
                       [1.5, 20.2],
                       [1.63, 34.4],
                       [10.9, 3.3]])

  # Create normalizer
  normalizer = Normalizer(norm="l2")

  # Transform feature matrix
  return (normalizer.transform(features))
#+END_SRC

#+RESULTS:
| 0.70710678 | 0.70710678 |
| 0.30782029 | 0.95144452 |
| 0.07405353 | 0.99725427 |
| 0.04733062 | 0.99887928 |
| 0.95709822 | 0.28976368 |

** Transforming Features
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import FunctionTransformer

  features = np.array([[2,3],[2,3],[2,3]])

  def add_ten(x):
      return x+10

  ten_transformer = FunctionTransformer(add_ten)

  return ten_transformer.transform(features)
#+END_SRC

#+RESULTS:
| 12 | 13 |
| 12 | 13 |
| 12 | 13 |

** Detecting Outliers
#+BEGIN_SRC python
  import numpy as np
  from sklearn.covariance import EllipticEnvelope
  from sklearn.datasets import make_blobs

  # Create simulated data
  features, _ = make_blobs(n_samples = 10,
                           n_features = 2,
                           centers = 1,
                           random_state = 1)

  # Replace the first observation's values with extreme values
  features[0,0] = 10000
  features[0,1] = 10000

  # Create detector
  outlier_detector = EllipticEnvelope(contamination=.1)

  # Fit detector
  outlier_detector.fit(features)

  # Predict outliers
  return outlier_detector.predict(features)
#+END_SRC

#+RESULTS:
| -1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |


A major limitation of this approach is the need to specify a
/contamination/ parameter, which is the proportion of observations that
are outliers - a value that we don't know.

If we expect our data to have few outliers, we can set /contamination/
to something small.

** Interquartile range (IQR)
#+BEGIN_SRC python
  import numpy as np
  from sklearn.covariance import EllipticEnvelope
  from sklearn.datasets import make_blobs
  
  # Create simulated data
  features, _ = make_blobs(n_samples = 10,
                           n_features = 2,
                           centers = 1,
                           random_state = 1)
  
  # Replace the first observation's values with extreme values
  features[0,0] = 10000
  features[0,1] = 10000
  
  # Create one feature
  feature = features[:,0]
  
  # Create a function to return index of outliers
  def indicies_of_outliers(x):
      q1, q3 = np.percentile(x, [25, 75])
      iqr = q3 - q1
      lower_bound = q1 - (iqr * 1.5)
      upper_bound = q3 + (iqr * 1.5)
      return np.where((x > upper_bound) | (x < lower_bound))
  
  # Run function
  return indicies_of_outliers(feature)
#+END_SRC

#+RESULTS:
| array | ((0)) |

** Handling Outliers
*** Drop them
#+BEGIN_SRC python
import pandas as pd

# Create DataFrame
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]

# Filter observations
return houses[houses['Bathrooms'] < 20]
#+END_SRC

#+RESULTS:
: Price  Bathrooms  Square_Feet
: 0  534433        2.0         1500
: 1  392333        3.5         2500
: 2  293222        2.0         1500

*** Mark
#+BEGIN_SRC python
import pandas as pd
import numpy as np

# Create DataFrame
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]
houses["Outlier"] = np.where(houses["Bathrooms"] < 20, 0, 1)
return houses
#+END_SRC

#+RESULTS:
: Price  Bathrooms  Square_Feet  Outlier
: 0   534433        2.0         1500        0
: 1   392333        3.5         2500        0
: 2   293222        2.0         1500        0
: 3  4322032      116.0        48000        1

*** Transform the feature to dampen the effect of the outlier
#+BEGIN_SRC python
import pandas as pd
import numpy as np

# Create DataFrame
houses = pd.DataFrame()
houses['Price'] = [534433, 392333, 293222, 4322032]
houses['Bathrooms'] = [2, 3.5, 2, 116]
houses['Square_Feet'] = [1500, 2500, 1500, 48000]
houses['Log_Of_Square_Feet'] = [np.log(x) for x in houses["Square_Feet"]]

return houses
#+END_SRC

#+RESULTS:
: Price  Bathrooms  Square_Feet  Log_Of_Square_Feet
: 0   534433        2.0         1500            7.313220
: 1   392333        3.5         2500            7.824046
: 2   293222        2.0         1500            7.313220
: 3  4322032      116.0        48000           10.778956

** Discretizating Features
*** Binarize the feature
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import Binarizer

  # Create feature
  age = np.array([[6],
                  [12],
                  [20],
                  [36],
                  [65]])

  # Create binarizer
  binarizer = Binarizer(18)

  # Transform feature
  return binarizer.fit_transform(age)
#+END_SRC

#+RESULTS:
| 0 |
| 0 |
| 1 |
| 1 |
| 1 |

*** Multiple thresholds
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import Binarizer

  # Create feature
  age = np.array([[6],
                  [12],
                  [20],
                  [36],
                  [65]])

  # Create binarizer
  return np.digitize(age, bins=[20,30,64], right=True)
#+END_SRC

#+RESULTS:
| 0 |
| 0 |
| 0 |
| 2 |
| 3 |

** Grouping Observations Using Clustering
#+BEGIN_SRC python
  import pandas as pd
  from sklearn.datasets import make_blobs
  from sklearn.cluster import KMeans

  features, _ = make_blobs(n_samples = 50,
                           n_features = 2,
                           centers = 3,
                           random_state = 1)

  dataframe = pd.DataFrame(features, columns=["feature_1", "feature_2"])
  clusterer = KMeans(3, random_state=0)
  clusterer.fit(features)

  dataframe["group"] = clusterer.predict(features)

  return dataframe.head(5)
#+END_SRC

#+RESULTS:
: feature_1  feature_2  group
: 0  -9.877554  -3.336145      0
: 1  -7.287210  -8.353986      2
: 2  -6.943061  -7.023744      2
: 3  -7.440167  -8.791959      2
: 4  -6.641388  -8.075888      2

** Predict missing values using k-nearest neighbors (KNN) and scikit-learn's Imputer
#+BEGIN_SRC python
import numpy as np
from fancyimpute import KNN
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs
from sklearn.preprocessing import Imputer

# Make a simulated feature matrix
features, _ = make_blobs(n_samples = 1000,
                         n_features = 2,
                         random_state = 1)

# Standardize the features
scaler = StandardScaler()
standardized_features = scaler.fit_transform(features)

# Replace the first feature's first value with a missing value
true_value = standardized_features[0,0]
standardized_features[0,0] = np.nan

# Predict the missing values in the feature matrix
features_knn_imputed = KNN(k=5, verbose=0).complete(standardized_features)

mean_imputer = Imputer(strategy="mean", axis=0)
features_mean_imputed = mean_imputer.fit_transform(standardized_features)

# Compare true and imputed values
return ("True Value:\t{}\nKNN Imputed Value:\t{}\nsklearn Imputed: {}\n".format(true_value, features_knn_imputed[0,0], features_mean_imputed[0,0]))
#+END_SRC

#+RESULTS:
: True Value:	0.8730186113995938
: KNN Imputed Value:	1.0955332713113226
: sklearn Imputed: -0.000873892503901796

scikit-learns' Imputer module typically gets worse results than KNN.

* Handling Categorical Data
When the classes have no intrisic ordering, numerical values
erroneously create an ordering that is not present.

The proper way is to create a binary feature for each class in the
original feature. This is often called *one-hot encoding* (in machine
learning literature) or *dummying* (in statistical and research
literature).

In digital circuits, *one-hot* is a group of bits among which the legal
combinations of values are only those with a single high (1) bit and
all the others low (0). A similar implementation in which all bits
are '1' except one '0' is sometimes called *one-cold*.

** Encoding Nominal Categorical Features
#+BEGIN_SRC python
import numpy as np
from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer

feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
# create one-hot encoder
one_hot = LabelBinarizer()
return one_hot.fit_transform(feature)
#+END_SRC

#+RESULTS:
| 0 | 0 | 1 |
| 1 | 0 | 0 |
| 0 | 0 | 1 |
| 0 | 1 | 0 |
| 0 | 0 | 1 |

Output the classes.
#+BEGIN_SRC python
import numpy as np
from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer

feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
# create one-hot encoder
one_hot = LabelBinarizer()
one_hot.fit_transform(feature)
return one_hot.classes_
#+END_SRC

#+RESULTS:
| California | Delaware | Texas |

Reverse the one-hot encoding.
#+BEGIN_SRC python
import numpy as np
from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer

feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
# create one-hot encoder
one_hot = LabelBinarizer()
return one_hot.inverse_transform(one_hot.fit_transform(feature))
#+END_SRC

#+RESULTS:
| Texas | California | Texas | Delaware | Texas |


Use pandas to one-hot encode the feature.
#+BEGIN_SRC python
import numpy as np
import pandas as pd
feature = np.array([["Texas"], ["California"], ["Texas"], ["Delaware"], ["Texas"]])
return pd.get_dummies(feature[:,0])
#+END_SRC

#+RESULTS:
: California  Delaware  Texas
: 0           0         0      1
: 1           1         0      0
: 2           0         0      1
: 3           0         1      0
: 4           0         0      1

Handle situations where each observation lists multiple classes.
#+BEGIN_SRC python
  import numpy as np
  from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer
  
  feature = [("Texas", "Florida"),
             ("California", "Alabama"),
             ("Texas", "Florida"),
             ("Delaware", "Florida"),
             ("Texas", "Alabama")]
  # create one-hot encoder
  one_hot_multiclass = MultiLabelBinarizer()
  return "encoded:\n{}\nclasses:\n{}\n".format(one_hot_multiclass.fit_transform(feature), one_hot_multiclass.classes_)
#+END_SRC

#+RESULTS:
: encoded:
: [[0 0 0 1 1]
:  [1 1 0 0 0]
:  [0 0 0 1 1]
:  [0 0 1 1 0]
:  [1 0 0 0 1]]
: classes:
: ['Alabama' 'California' 'Delaware' 'Florida' 'Texas']

It is often recommended that after one-hot encoding a feature, we drop
one of the one-hot features in the resulting matrix to avoid linear
dependence.

** Encoding Ordinal Categorical Features
Often we have a feature with classes that have some kind of natural
ordering. A famous example is the *Likert scale*: Strongly Agree, Agree,
Neutral, Disagree, Strongly Disagree

Use pandas DataFrame's *replace* method to transform string labels to
numerical equivalents.

#+BEGIN_SRC python
import pandas as pd
dataframe = pd.DataFrame({"Score": ["Low", "Low", "Medium", "Medium", "High"]})
scale_mapper = {"Low": 1, "Medium": 2, "High": 3}
# replace feature values with scale
return dataframe["Score"].replace(scale_mapper)
#+END_SRC

#+RESULTS:
: 0    1
: 1    1
: 2    2
: 3    2
: 4    3
: Name: Score, dtype: int64

** Encoding Dictionaries of Features
Use *DictVectorizer*.
#+BEGIN_SRC python
  from sklearn.feature_extraction import DictVectorizer
  data_dict = [{"Red": 2, "Blue": 4},
               {"Red": 4, "Blue": 3},
               {"Red": 1, "Yellow": 2},
               {"Red": 2, "Yellow": 2}]
  dict_vectorizer = DictVectorizer(sparse=False)
  features = dict_vectorizer.fit_transform(data_dict)
  feature_names = dict_vectorizer.get_feature_names()
  return "features:\n{}\nnames:\n{}\n".format(features, feature_names)
#+END_SRC

#+RESULTS:
: features:
: [[4. 2. 0.]
:  [3. 4. 0.]
:  [0. 1. 2.]
:  [0. 2. 2.]]
: names:
: ['Blue', 'Red', 'Yellow']

** Imputing Missing Class Values
#+BEGIN_SRC python
  import numpy as np
  from sklearn.neighbors import KNeighborsClassifier
  from sklearn.preprocessing import Imputer
  
  X = np.array([[0, 2.10, 1.45],
                [1, 1.18, 1.33],
                [0, 1.22, 1.27],
                [1, -0.21, -1.19]])
  X_with_nan = np.array([[np.nan, 0.87, 1.31],
                         [np.nan, -0.67, -0.22]])
  
  # Train KNN learner
  clf = KNeighborsClassifier(3, weights="distance")
  trained_model = clf.fit(X[:, 1:], X[:,0])
  
  # Predict missing values' class
  imputed_values = trained_model.predict(X_with_nan[:, 1:])
  
  # Join column of predicted class with their other features
  X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:, 1:]))
  
  # Join two feature matrices
  knc = np.vstack((X_with_imputed, X))
  
  # Fill in missing values with the feature's most frequent value
  X_complete = np.vstack((X_with_nan, X))
  imputer = Imputer(strategy="most_frequent", axis=0)
  mfv = imputer.fit_transform(X_complete)
  
  return "KNeighborsClassifier:\n{}\nmost frequent values:\n{}\n".format(knc, mfv)
#+END_SRC

#+RESULTS:
#+begin_example
KNeighborsClassifier:
[[ 0.    0.87  1.31]
 [ 1.   -0.67 -0.22]
 [ 0.    2.1   1.45]
 [ 1.    1.18  1.33]
 [ 0.    1.22  1.27]
 [ 1.   -0.21 -1.19]]
most frequent values:
[[ 0.    0.87  1.31]
 [ 0.   -0.67 -0.22]
 [ 0.    2.1   1.45]
 [ 1.    1.18  1.33]
 [ 0.    1.22  1.27]
 [ 1.   -0.21 -1.19]]
#+end_example

** Handling Imbalanced Classes
Many algorithms in scikit-learn offer a prameter to weight classes
during training to counteract the effect of their imbalance.
#+BEGIN_SRC python
  import numpy as np
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.datasets import load_iris
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # remove first 40 observations
  features = features[40:,:]
  target = target[40:]
  
  # create binary target vector indicating if class 0
  target = np.where((target == 0), 0, 1)
  
  # use weights  
  weights = {0: .9, 1: 0.1}
  RandomForestClassifier(class_weight=weights)
  
  # create weights inversely proportional to class frequencies
  RandomForestClassifier(class_weight="balanced")
  
  # downsample the majority class
  i_class0 = np.where(target == 0)[0]
  i_class1 = np.where(target == 1)[0]
  n_class0 = len(i_class0)
  n_class1 = len(i_class1)
  i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)
  np.hstack((target[i_class0], target[i_class1_downsampled]))
  np.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]
  
  # upsample the minority class
  i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)
  np.concatenate((target[i_class0_upsampled], target[i_class1]))
  np.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]
#+END_SRC
*** downsample the majority class
#+BEGIN_SRC python
  import numpy as np
  from sklearn.datasets import load_iris
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # remove first 40 observations
  features = features[40:,:]
  target = target[40:]
  
  # create binary target vector indicating if class 0
  target = np.where((target == 0), 0, 1)
  
  # downsample the majority class
  i_class0 = np.where(target == 0)[0]
  i_class1 = np.where(target == 1)[0]
  n_class0 = len(i_class0)
  n_class1 = len(i_class1)
  i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)
  # join target vector
  target_vector = np.hstack((target[i_class0], target[i_class1_downsampled]))
  # join feature matrix
  feature_matrix = np.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]
  
  return "target vector:\n{}\nfeature matrix:\n{}\n".format(target_vector, feature_matrix)
#+END_SRC

#+RESULTS:
: target vector:
: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]
: feature matrix:
: [[5.  3.5 1.3 0.3]
:  [4.5 2.3 1.3 0.3]
:  [4.4 3.2 1.3 0.2]
:  [5.  3.5 1.6 0.6]
:  [5.1 3.8 1.9 0.4]]

*** upsample the minority class
#+BEGIN_SRC python
  import numpy as np
  from sklearn.datasets import load_iris
  
  iris = load_iris()
  features = iris.data
  target = iris.target
  
  # remove first 40 observations
  features = features[40:,:]
  target = target[40:]
  
  # create binary target vector indicating if class 0
  target = np.where((target == 0), 0, 1)

  i_class0 = np.where(target == 0)[0]
  i_class1 = np.where(target == 1)[0]
  n_class0 = len(i_class0)
  n_class1 = len(i_class1)
  
  i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)
  # join target vector
  target_vector = np.concatenate((target[i_class0_upsampled], target[i_class1]))
  # join feature matrix
  feature_matrix = np.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]
  return "target vector:\n{}\nfeature matrix:\n{}\n".format(target_vector, feature_matrix)  
#+END_SRC

#+RESULTS:
#+begin_example
  target vector:
  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
   0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
   1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
  feature matrix:
  [[4.6 3.2 1.4 0.2]
   [5.1 3.8 1.6 0.2]
   [4.4 3.2 1.3 0.2]
   [5.1 3.8 1.9 0.4]
   [5.  3.3 1.4 0.2]]
#+end_example

* Handling Text
** Cleaning Text
Use python's core string operations.
#+BEGIN_SRC python
  # Create text
  text_data = ["   Interrobang. By Aishwarya Henriette     ",
               "Parking And Going. By Karl Gautier",
               "    Today Is The night. By Jarek Prakash   "]
  # Strip whitespaces
  strip_whitespace = [string.strip() for string in text_data]
  
  def capitalizer(string: str) -> str:
      return string.upper()
  
  # Remove periods
  remove_periods = [capitalizer(string.replace(".","")) for string in strip_whitespace]
  
  return remove_periods
#+END_SRC

#+RESULTS:
| INTERROBANG BY AISHWARYA HENRIETTE | PARKING AND GOING BY KARL GAUTIER | TODAY IS THE NIGHT BY JAREK PRAKASH |

Regular expressions:
#+BEGIN_SRC python
  import re
  
  # Create text
  text_data = ["   Interrobang. By Aishwarya Henriette     ",
               "Parking And Going. By Karl Gautier",
               "    Today Is The night. By Jarek Prakash   "]
  # Strip whitespaces
  strip_whitespace = [string.strip() for string in text_data]
  
  # Remove periods
  remove_periods = [string.replace(".","") for string in strip_whitespace]
  
  def replace_letters_with_X(string: str) -> str:
      return re.sub(r"[a-zA-Z]", "X", string)
  
  replaced = [replace_letters_with_X(string) for string in remove_periods]
  return replaced
#+END_SRC

#+RESULTS:
| XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX | XXXXXXX XXX XXXXX XX XXXX XXXXXXX | XXXXX XX XXX XXXXX XX XXXXX XXXXXXX |

** Parsing and cleaning HTML
Use Beautiful Soup's extensive set of options to parse and extract from HTML.
https://www.crummy.com/software/BeautifulSoup/bs4/doc/>
#+BEGIN_SRC python
  from bs4 import BeautifulSoup
  
  html = """
  <div class='full_name'>
          <span style='font-weight:bold'>Masego</span> Azra
  </div>
  """
  
  soup = BeautifulSoup(html, "lxml")
  return soup.find("div", { "class": "full_name" }).text
#+END_SRC

#+RESULTS:
: Masego Azra

** Removing punctuation
Use *translate* with a dictionary of punctuation characters.
http://python-reference.readthedocs.io/en/latest/docs/str/translate.html
https://www.tutorialspoint.com/python/dictionary_fromkeys.htm
#+BEGIN_SRC python
  import unicodedata
  import sys
  
  text_data = ['Hi!!!! I. Love. This. Song....',
               '10000% Agree!!!! #LoveIT',
               'Right?!?!']
  punctuation = dict.fromkeys(i for i in range(sys.maxunicode)
                              if unicodedata.category(chr(i)).startswith('P'))
  
  translated = [string.translate(punctuation) for string in text_data]
  
  return "punctuation:\n{}\ntranslated:\n{}\n".format(punctuation, translated)
#+END_SRC

#+RESULTS:
: punctuation:
: {33: None, 34: None, 35: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 63: None, 64: None, 91: None, 92: None, 93: None, 95: None, 123: None, 125: None, 161: None, 167: None, 171: None, 182: None, 183: None, 187: None, 191: None, 894: None, 903: None, 1370: None, 1371: None, 1372: None, 1373: None, 1374: None, 1375: None, 1417: None, 1418: None, 1470: None, 1472: None, 1475: None, 1478: None, 1523: None, 1524: None, 1545: None, 1546: None, 1548: None, 1549: None, 1563: None, 1566: None, 1567: None, 1642: None, 1643: None, 1644: None, 1645: None, 1748: None, 1792: None, 1793: None, 1794: None, 1795: None, 1796: None, 1797: None, 1798: None, 1799: None, 1800: None, 1801: None, 1802: None, 1803: None, 1804: None, 1805: None, 2039: None, 2040: None, 2041: None, 2096: None, 2097: None, 2098: None, 2099: None, 2100: None, 2101: None, 2102: None, 2103: None, 2104: None, 2105: None, 2106: None, 2107: None, 2108: None, 2109: None, 2110: None, 2142: None, 2404: None, 2405: None, 2416: None, 2800: None, 3572: None, 3663: None, 3674: None, 3675: None, 3844: None, 3845: None, 3846: None, 3847: None, 3848: None, 3849: None, 3850: None, 3851: None, 3852: None, 3853: None, 3854: None, 3855: None, 3856: None, 3857: None, 3858: None, 3860: None, 3898: None, 3899: None, 3900: None, 3901: None, 3973: None, 4048: None, 4049: None, 4050: None, 4051: None, 4052: None, 4057: None, 4058: None, 4170: None, 4171: None, 4172: None, 4173: None, 4174: None, 4175: None, 4347: None, 4960: None, 4961: None, 4962: None, 4963: None, 4964: None, 4965: None, 4966: None, 4967: None, 4968: None, 5120: None, 5741: None, 5742: None, 5787: None, 5788: None, 5867: None, 5868: None, 5869: None, 5941: None, 5942: None, 6100: None, 6101: None, 6102: None, 6104: None, 6105: None, 6106: None, 6144: None, 6145: None, 6146: None, 6147: None, 6148: None, 6149: None, 6150: None, 6151: None, 6152: None, 6153: None, 6154: None, 6468: None, 6469: None, 6686: None, 6687: None, 6816: None, 6817: None, 6818: None, 6819: None, 6820: None, 6821: None, 6822: None, 6824: None, 6825: None, 6826: None, 6827: None, 6828: None, 6829: None, 7002: None, 7003: None, 7004: None, 7005: None, 7006: None, 7007: None, 7008: None, 7164: None, 7165: None, 7166: None, 7167: None, 7227: None, 7228: None, 7229: None, 7230: None, 7231: None, 7294: None, 7295: None, 7360: None, 7361: None, 7362: None, 7363: None, 7364: None, 7365: None, 7366: None, 7367: None, 7379: None, 8208: None, 8209: None, 8210: None, 8211: None, 8212: None, 8213: None, 8214: None, 8215: None, 8216: None, 8217: None, 8218: None, 8219: None, 8220: None, 8221: None, 8222: None, 8223: None, 8224: None, 8225: None, 8226: None, 8227: None, 8228: None, 8229: None, 8230: None, 8231: None, 8240: None, 8241: None, 8242: None, 8243: None, 8244: None, 8245: None, 8246: None, 8247: None, 8248: None, 8249: None, 8250: None, 8251: None, 8252: None, 8253: None, 8254: None, 8255: None, 8256: None, 8257: None, 8258: None, 8259: None, 8261: None, 8262: None, 8263: None, 8264: None, 8265: None, 8266: None, 8267: None, 8268: None, 8269: None, 8270: None, 8271: None, 8272: None, 8273: None, 8275: None, 8276: None, 8277: None, 8278: None, 8279: None, 8280: None, 8281: None, 8282: None, 8283: None, 8284: None, 8285: None, 8286: None, 8317: None, 8318: None, 8333: None, 8334: None, 8968: None, 8969: None, 8970: None, 8971: None, 9001: None, 9002: None, 10088: None, 10089: None, 10090: None, 10091: None, 10092: None, 10093: None, 10094: None, 10095: None, 10096: None, 10097: None, 10098: None, 10099: None, 10100: None, 10101: None, 10181: None, 10182: None, 10214: None, 10215: None, 10216: None, 10217: None, 10218: None, 10219: None, 10220: None, 10221: None, 10222: None, 10223: None, 10627: None, 10628: None, 10629: None, 10630: None, 10631: None, 10632: None, 10633: None, 10634: None, 10635: None, 10636: None, 10637: None, 10638: None, 10639: None, 10640: None, 10641: None, 10642: None, 10643: None, 10644: None, 10645: None, 10646: None, 10647: None, 10648: None, 10712: None, 10713: None, 10714: None, 10715: None, 10748: None, 10749: None, 11513: None, 11514: None, 11515: None, 11516: None, 11518: None, 11519: None, 11632: None, 11776: None, 11777: None, 11778: None, 11779: None, 11780: None, 11781: None, 11782: None, 11783: None, 11784: None, 11785: None, 11786: None, 11787: None, 11788: None, 11789: None, 11790: None, 11791: None, 11792: None, 11793: None, 11794: None, 11795: None, 11796: None, 11797: None, 11798: None, 11799: None, 11800: None, 11801: None, 11802: None, 11803: None, 11804: None, 11805: None, 11806: None, 11807: None, 11808: None, 11809: None, 11810: None, 11811: None, 11812: None, 11813: None, 11814: None, 11815: None, 11816: None, 11817: None, 11818: None, 11819: None, 11820: None, 11821: None, 11822: None, 11824: None, 11825: None, 11826: None, 11827: None, 11828: None, 11829: None, 11830: None, 11831: None, 11832: None, 11833: None, 11834: None, 11835: None, 11836: None, 11837: None, 11838: None, 11839: None, 11840: None, 11841: None, 11842: None, 11843: None, 11844: None, 12289: None, 12290: None, 12291: None, 12296: None, 12297: None, 12298: None, 12299: None, 12300: None, 12301: None, 12302: None, 12303: None, 12304: None, 12305: None, 12308: None, 12309: None, 12310: None, 12311: None, 12312: None, 12313: None, 12314: None, 12315: None, 12316: None, 12317: None, 12318: None, 12319: None, 12336: None, 12349: None, 12448: None, 12539: None, 42238: None, 42239: None, 42509: None, 42510: None, 42511: None, 42611: None, 42622: None, 42738: None, 42739: None, 42740: None, 42741: None, 42742: None, 42743: None, 43124: None, 43125: None, 43126: None, 43127: None, 43214: None, 43215: None, 43256: None, 43257: None, 43258: None, 43260: None, 43310: None, 43311: None, 43359: None, 43457: None, 43458: None, 43459: None, 43460: None, 43461: None, 43462: None, 43463: None, 43464: None, 43465: None, 43466: None, 43467: None, 43468: None, 43469: None, 43486: None, 43487: None, 43612: None, 43613: None, 43614: None, 43615: None, 43742: None, 43743: None, 43760: None, 43761: None, 44011: None, 64830: None, 64831: None, 65040: None, 65041: None, 65042: None, 65043: None, 65044: None, 65045: None, 65046: None, 65047: None, 65048: None, 65049: None, 65072: None, 65073: None, 65074: None, 65075: None, 65076: None, 65077: None, 65078: None, 65079: None, 65080: None, 65081: None, 65082: None, 65083: None, 65084: None, 65085: None, 65086: None, 65087: None, 65088: None, 65089: None, 65090: None, 65091: None, 65092: None, 65093: None, 65094: None, 65095: None, 65096: None, 65097: None, 65098: None, 65099: None, 65100: None, 65101: None, 65102: None, 65103: None, 65104: None, 65105: None, 65106: None, 65108: None, 65109: None, 65110: None, 65111: None, 65112: None, 65113: None, 65114: None, 65115: None, 65116: None, 65117: None, 65118: None, 65119: None, 65120: None, 65121: None, 65123: None, 65128: None, 65130: None, 65131: None, 65281: None, 65282: None, 65283: None, 65285: None, 65286: None, 65287: None, 65288: None, 65289: None, 65290: None, 65292: None, 65293: None, 65294: None, 65295: None, 65306: None, 65307: None, 65311: None, 65312: None, 65339: None, 65340: None, 65341: None, 65343: None, 65371: None, 65373: None, 65375: None, 65376: None, 65377: None, 65378: None, 65379: None, 65380: None, 65381: None, 65792: None, 65793: None, 65794: None, 66463: None, 66512: None, 66927: None, 67671: None, 67871: None, 67903: None, 68176: None, 68177: None, 68178: None, 68179: None, 68180: None, 68181: None, 68182: None, 68183: None, 68184: None, 68223: None, 68336: None, 68337: None, 68338: None, 68339: None, 68340: None, 68341: None, 68342: None, 68409: None, 68410: None, 68411: None, 68412: None, 68413: None, 68414: None, 68415: None, 68505: None, 68506: None, 68507: None, 68508: None, 69703: None, 69704: None, 69705: None, 69706: None, 69707: None, 69708: None, 69709: None, 69819: None, 69820: None, 69822: None, 69823: None, 69824: None, 69825: None, 69952: None, 69953: None, 69954: None, 69955: None, 70004: None, 70005: None, 70085: None, 70086: None, 70087: None, 70088: None, 70089: None, 70093: None, 70107: None, 70109: None, 70110: None, 70111: None, 70200: None, 70201: None, 70202: None, 70203: None, 70204: None, 70205: None, 70313: None, 70731: None, 70732: None, 70733: None, 70734: None, 70735: None, 70747: None, 70749: None, 70854: None, 71105: None, 71106: None, 71107: None, 71108: None, 71109: None, 71110: None, 71111: None, 71112: None, 71113: None, 71114: None, 71115: None, 71116: None, 71117: None, 71118: None, 71119: None, 71120: None, 71121: None, 71122: None, 71123: None, 71124: None, 71125: None, 71126: None, 71127: None, 71233: None, 71234: None, 71235: None, 71264: None, 71265: None, 71266: None, 71267: None, 71268: None, 71269: None, 71270: None, 71271: None, 71272: None, 71273: None, 71274: None, 71275: None, 71276: None, 71484: None, 71485: None, 71486: None, 72769: None, 72770: None, 72771: None, 72772: None, 72773: None, 72816: None, 72817: None, 74864: None, 74865: None, 74866: None, 74867: None, 74868: None, 92782: None, 92783: None, 92917: None, 92983: None, 92984: None, 92985: None, 92986: None, 92987: None, 92996: None, 113823: None, 121479: None, 121480: None, 121481: None, 121482: None, 121483: None, 125278: None, 125279: None}
: translated:
: ['Hi I Love This Song', '10000 Agree LoveIT', 'Right']

** Tokenizing text
Natural Language Toolkit for Python (NLTK) has a powerful set of text
manipulation operations, including word tokenizing.

#+BEGIN_SRC python
  from nltk.tokenize import word_tokenize
  from nltk.tokenize import sent_tokenize
  
  string = "The science of today is the technology of tomorrow. Tomorrow is today."
  return "words:\n{}\nsentences:\n{}\n".format(word_tokenize(string), sent_tokenize(string))
#+END_SRC

#+RESULTS:
: words:
: ['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow', '.', 'Tomorrow', 'is', 'today', '.']
: sentences:
: ['The science of today is the technology of tomorrow.', 'Tomorrow is today.']

** Removing stop words
#+BEGIN_SRC python
  from nltk.corpus import stopwords
  
  # You will have to download the set of stop words the first time
  # import nltk
  # nltk.download('stopwords')
  
  tokenzied_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']
  stop_words = stopwords.words('english')
  
  #remove stop words
  return [word for word in tokenzied_words if word not in stop_words]
#+END_SRC

#+RESULTS:
| going | go | store | park |

Note that NLTK's stopwords assumes the tokenized words are all lowercased.

** Stemming words
NLTK's PorterStemmer implements the widely used Porter stemming
algorithm to remove or replace common suffixes to produce the word
stem.
#+BEGIN_SRC python
  from nltk.stem.porter import PorterStemmer
  
  tokenzied_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']
  porter = PorterStemmer()
  return [porter.stem(word) for word in tokenzied_words]
#+END_SRC

#+RESULTS:
| i | am | humbl | by | thi | tradit | meet |


** Tagging parts of speech
Use NLTK's pre-trained parts-of-speech tagger.
#+BEGIN_SRC python
  from nltk import pos_tag
  from nltk import word_tokenize
  
  text_data = "Chris loverd outdoor running"
  text_tagged = pos_tag(word_tokenize(text_data))
  return text_tagged
#+END_SRC

#+RESULTS:
| Chris   | NNP |
| loverd  | JJ  |
| outdoor | NN  |
| running | VBG |

NLTK uses the Penn Treebank parts for speech tags. Some examples of
the Penn Treebank tags are.
| Tag | Part of speech                     |
|-----+------------------------------------|
| NNP | Proper noun, singular              |
| NN  | Noun, singular or mass             |
| RB  | Adverb                             |
| VBD | Verb, past tense                   |
| VBG | Verb, gerund or present participle |
| JJ  | Adjective                          |
| PRP | Personal pronoun                   |

#+BEGIN_SRC python
  from sklearn.preprocessing import MultiLabelBinarizer
  from nltk import pos_tag
  from nltk import word_tokenize
  
  tweets = ["I am eating a burrito for breakfast",
            "Political science is an amazing field",
            "San Francisco is an awesome city"]
  tagged_tweets = []
  for tweet in tweets:
      tweet_tag = pos_tag(word_tokenize(tweet))
      tagged_tweets.append([tag for word, tag in tweet_tag])
  
  one_hot_multi = MultiLabelBinarizer()
  features = one_hot_multi.fit_transform(tagged_tweets)
  return "features:\n{}\nclasses:\n{}\n".format(features, one_hot_multi.classes_)
#+END_SRC

#+RESULTS:
: features:
: [[1 1 0 1 0 1 1 1 0]
:  [1 0 1 1 0 0 0 0 1]
:  [1 0 1 1 1 0 0 0 1]]
: classes:
: ['DT' 'IN' 'JJ' 'NN' 'NNP' 'PRP' 'VBG' 'VBP' 'VBZ']

NLTK also gives us the ability to train our own tagger.
#+BEGIN_SRC python
  from nltk.corpus import brown
  from nltk.tag import UnigramTagger
  from nltk.tag import BigramTagger
  from nltk.tag import TrigramTagger
  
  sentences = brown.tagged_sents(categories='news')
  train = sentences[:4000]
  test = sentences[4000:]
  
  # create backoff tagger
  unigram = UnigramTagger(train)
  bigram = BigramTagger(train, backoff=unigram)
  trigram = TrigramTagger(train, backoff=bigram)
  
  # show accuracy
  return trigram.evaluate(test)
#+END_SRC

#+RESULTS:
: 0.8174734002697437

** Encoding text as a bag of words
Create a set of features indicating the number of times an
observation's text contains a particular word.

#+BEGIN_SRC python
  import numpy as np
  from sklearn.feature_extraction.text import CountVectorizer
  
  text_data = np.array(['I love Brazil. Brazil!',
                        'Sweden is best',
                        'Germany beats both'])
  # create a bag of words feature matrix
  count = CountVectorizer()
  bag_of_words = count.fit_transform(text_data)
  feature_names = count.get_feature_names()
  
  return "bag of words:\n{}\nto array:\n{}\nfeature names:\n{}\nvocabulary:\n{}\n".format(bag_of_words, bag_of_words.toarray(), feature_names, count.vocabulary_)
#+END_SRC

#+RESULTS:
#+begin_example
bag of words:
  (0, 3)	2
  (0, 6)	1
  (1, 1)	1
  (1, 5)	1
  (1, 7)	1
  (2, 2)	1
  (2, 0)	1
  (2, 4)	1
to array:
[[0 0 0 2 0 0 1 0]
 [0 1 0 0 0 1 0 1]
 [1 0 1 0 1 0 0 0]]
feature names:
['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']
vocabulary:
{'love': 6, 'brazil': 3, 'sweden': 7, 'is': 5, 'best': 1, 'germany': 4, 'beats': 0, 'both': 2}
#+end_example

Useful parameters of CountVectorizer.
#+BEGIN_SRC python
# Create feature matrix with arguments
count_2gram = CountVectorizer(ngram_range=(1,2),
                              stop_words="english",
                              vocabulary=['brazil'])
bag = count_2gram.fit_transform(text_data)

# View feature matrix
bag.toarray()
#+END_SRC

** Weighting word importance
Compare the frequency of the word in a document (a tweet, movie
review, speech transcript, etc.) with the frequency of the word in all
other documents using term frequency-inverse document frequency
(tf-idf).

#+BEGIN_SRC python
  import numpy as np
  from sklearn.feature_extraction.text import TfidfVectorizer
  
  text_data = np.array(['I love Brazil. Brazil!',
                        'Sweden is best',
                        'Germany beats both'])
  tfidf = TfidfVectorizer()
  feature_matrix = tfidf.fit_transform(text_data)
  return "feature matrix:\n{}\nvocabulary:\n{}\n".format(feature_matrix.toarray(), tfidf.vocabulary_)
#+END_SRC

#+RESULTS:
: feature matrix:
: [[0.         0.         0.         0.89442719 0.         0.
:   0.4472136  0.        ]
:  [0.         0.57735027 0.         0.         0.         0.57735027
:   0.         0.57735027]
:  [0.57735027 0.         0.57735027 0.         0.57735027 0.
:   0.         0.        ]]
: vocabulary:
: {'love': 6, 'brazil': 3, 'sweden': 7, 'is': 5, 'best': 1, 'germany': 4, 'beats': 0, 'both': 2}

The more a word appears in a document, the more likely it is important
to that document. For example, if the word /economy/ appears frequently,
it is evidence that the document might be about economics. We call
this *term frequency (tf)*.

In contrast, if a word appears in many documents, it is likely less
important to any individual document. For example, if every document
in some text data contains the word /after/ then it is probably an
unimportant word. We call this *document frequency (df)*.

By combining these two statistics, we can assign a score to every word
representing how important that word is in a document. Specifically,
we multiply *tf* to the inverse of document frequency (*idf*):

tf-idf(t,d)=tf(t,d) x idf(t)

where *t* is a word and *d* is a document. There are a number of
variations in how *tf* and *idf* are calculated. In scikit-learn, *tf* is
simply the number of times a word appears in the document and *idf* is
calculated as:

idf(t)=log[ (1+nd) / (1 + df(d,t))] + 1

where *nd* is the number of documents and *df(d,t)* is term, t's document
frequency (i.e., number of documents where the term appears).

By default, scikit-learn then normalizes the tf-idf vectors using the
Euclidean norm (L2 norm). The higher the resulting value, the more
important the word is to a document.

* Handling dates and times
** Converting strings to dates
#+BEGIN_SRC python
  import numpy as np
  import pandas as pd
  
  date_strings = np.array(['03-04-2018 11:35 PM',
                           '24-05-2018 12:01 AM',
                           '08-09-2017 09:09 PM'])
  dates = [pd.to_datetime(date, format='%d-%m-%Y %I:%M %p') for date in date_strings]
  return dates
#+END_SRC

#+RESULTS:
| Timestamp | (2018-04-03 23:35:00) | Timestamp | (2018-05-24 00:01:00) | Timestamp | (2017-09-08 21:09:00) |

